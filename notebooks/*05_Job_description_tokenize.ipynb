{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T23:22:53.817308Z",
     "start_time": "2019-11-06T23:21:37.158885Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-12T07:14:52.093841Z",
     "start_time": "2019-11-12T07:14:48.656385Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import textcleaner\n",
    "import pickle\n",
    "import spacy\n",
    "import jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob import TextBlob\n",
    "import textcleaner as tc\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T23:42:08.963437Z",
     "start_time": "2019-11-08T23:42:08.756164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greenapple/anaconda3/envs/project4e/lib/python3.7/site-packages/ipykernel_launcher.py:25: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load scraped job description data\n",
    "\n",
    "file_list = [\n",
    "'data_110619_seattle_0to1k.pkl',\n",
    "'data_110619_seattle_1kto2k.pkl',\n",
    "'data_110619_seattle_2kto3k.pkl',\n",
    "'data_110619_seattle_neg_5.pkl',\n",
    "'data_110619_seattle_3kto4k.pkl',\n",
    "'data_110619_seattle_4ktoend.pkl',\n",
    "'data_110619_bellevue_0to1k.pkl',\n",
    "'data_110619_bellevue_1to2k.pkl',\n",
    "'data_110619_bellevue_2to3k.pkl',\n",
    "'data_110619_bellevue_3toend.pkl',\n",
    "'data_110619_seattle_neg.pkl'\n",
    "            ]\n",
    "jobs = pd.DataFrame()\n",
    "\n",
    "for file in file_list:\n",
    "    \n",
    "    folder = '/Users/greenapple/project4/data/interim/'\n",
    "    path = os.path.join(folder, file)\n",
    "    \n",
    "    pickling_out = open(path, 'rb')\n",
    "    df_piece = pickle.load(pickling_out)\n",
    "    jobs = pd.concat([jobs, df_piece]) \n",
    "    \n",
    "    if file == 'data_110619_seattle_neg.pkl':\n",
    "        jobs['type'] = 'negative'\n",
    "    else:\n",
    "        jobs['type'] = 'positive'\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T23:42:09.196142Z",
     "start_time": "2019-11-08T23:42:09.187960Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13029, 7)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T23:42:10.203177Z",
     "start_time": "2019-11-08T23:42:10.028159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13003, 7) (8937, 7)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and data rows that were inserted by indeed\n",
    "jobs.drop_duplicates(inplace=True)\n",
    "a = jobs.shape\n",
    "jobs.drop(jobs.loc[jobs.company_name == 'Seen by Indeed'].index, inplace=True)\n",
    "jobs.reset_index(drop=True, inplace=True)\n",
    "b = jobs.shape\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T18:22:32.494670Z",
     "start_time": "2019-11-08T18:22:32.490785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect data for the rows replaced by indeed\n",
    "# replaced_data_0to1k = data_110619_seattle_set1.loc[data_110619_seattle_set1.company_name=='Seen by Indeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:12:59.978068Z",
     "start_time": "2019-11-08T00:12:59.973786Z"
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T22:51:41.984355Z",
     "start_time": "2019-11-08T22:51:41.977172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-process text\n",
    "def pre_process(text):\n",
    "    '''\n",
    "    Takes a string, returns a cleaned string.\n",
    "    '''\n",
    "    text_sub = re.sub('[/.\\n:-]', ' ', text)  # Remove seaparators of dif kind\n",
    "    text_sub = re.sub('[,\\(\\)]', '', text_sub).lower()  # Remove \",\" and capital letters\n",
    "    text_sub = re.sub('[^a-z\\s]', '', text_sub) # Keep alphanumeric characters only\n",
    "#     re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "\n",
    "    return text_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T22:50:35.354479Z",
     "start_time": "2019-11-08T22:50:35.346247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-process text\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_numeric, strip_short, stem_text, strip_tags, strip_punctuation, strip_multiple_whitespaces, remove_stopwords\n",
    "\n",
    "def gensim_preprocess(text):\n",
    "    '''\n",
    "    Takes a string, returns a list of cleaned strings.\n",
    "    '''\n",
    "    filters = [\n",
    "        strip_tags,\n",
    "        strip_punctuation, \n",
    "        strip_multiple_whitespaces, \n",
    "        strip_numeric, \n",
    "        remove_stopwords, \n",
    "        strip_short,\n",
    "        lambda x: x.lower()]\n",
    "    \n",
    "    text_proc = preprocess_string(text, filters)\n",
    "    return text_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T18:23:05.641772Z",
     "start_time": "2019-11-08T18:23:05.635605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "def tokenize_lemma(text):\n",
    "        \n",
    "    # processer = spacy.load('en')\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "    text_obj = tokenizer(text)\n",
    "    text_str = ' '.join([token.lemma_ for token in text_obj if not token.is_stop])\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:18:01.514013Z",
     "start_time": "2019-11-09T03:18:00.805087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text: nouns\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_noun(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_noun = ' '.join([token.text for token in text_obj if token.pos_=='NOUN'])\n",
    "  \n",
    "    return text_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:22:37.742137Z",
     "start_time": "2019-11-09T03:22:37.170319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text: lemma and nouns\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_noun_lemma(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_noun_lemma = ' '.join([token.lemma_ for token in text_obj if token.pos_=='NOUN'])\n",
    "   \n",
    "  \n",
    "    return text_noun_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:31:13.698484Z",
     "start_time": "2019-11-09T03:31:13.065123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text: lemma\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_lemma(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_lemma = ' '.join([token.lemma_ for token in text_obj if not token.is_stop])\n",
    "  \n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:31:15.075949Z",
     "start_time": "2019-11-09T03:31:14.528850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text: words\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_word(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_text = ' '.join([token.text for token in text_obj if not token.is_stop])\n",
    "  \n",
    "    return text_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T00:45:36.218704Z",
     "start_time": "2019-11-09T00:45:34.859245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean\n",
    "jobs['clean_text'] = jobs['description'].apply(pre_process) # clean\n",
    "\n",
    "# Save cleaned text\n",
    "pickling_in = open('/Users/greenapple/project4/data/processed/jobs_clean.pkl', 'wb')\n",
    "pickle.dump(jobs, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:39:26.583982Z",
     "start_time": "2019-11-09T03:36:57.692068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize:\n",
    "jobs['noun'] = jobs['clean_text'].apply(tokenize_noun)\n",
    "jobs['noun_lemma'] = jobs['clean_text'].apply(tokenize_noun_lemma)\n",
    "jobs['lemma'] = jobs['clean_text'].apply(tokenize_lemma)\n",
    "jobs['word'] = jobs['clean_text'].apply(tokenize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:39:37.502898Z",
     "start_time": "2019-11-09T03:39:37.493782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company_name', 'description', 'job_title', 'link', 'location',\n",
       "       'salary', 'type', 'clean_text', 'noun', 'noun_lemma', 'lemma', 'word'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:39:46.182869Z",
     "start_time": "2019-11-09T03:39:45.901298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save tokenized text\n",
    "pickling_in = open('/Users/greenapple/project4/data/processed/jobs_tokenized.pkl', 'wb')\n",
    "pickle.dump(jobs, pickling_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project4e]",
   "language": "python",
   "name": "conda-env-project4e-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

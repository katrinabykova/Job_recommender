{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:08.888512Z",
     "start_time": "2019-11-13T21:57:07.758547Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:08.900740Z",
     "start_time": "2019-11-13T21:57:08.890839Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_job_urls(url, web_site):\n",
    "    '''\n",
    "    Returns urls for jobs posted on a page.\n",
    "    '''\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    elements = soup.find_all('div', {'class':'title'})\n",
    "    \n",
    "    job_urls = [el.find('a').get('href') for el in elements]\n",
    "    job_urls = [web_site + url for url in job_urls]\n",
    "    \n",
    "    return job_urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:09.093524Z",
     "start_time": "2019-11-13T21:57:09.085185Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_pages_urls(start_page):\n",
    "    '''\n",
    "    Returns urls for indeed pages with multiple job postings. \n",
    "    '''\n",
    "    page_urls = [start_page]\n",
    "    \n",
    "    for i in range(10, 3000, 10): #change to 3,000\n",
    "        page = os.path.join('{}&start={}'.format(start_page, i))\n",
    "        page_urls.append(page)\n",
    "        \n",
    "    return page_urls     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:09.539987Z",
     "start_time": "2019-11-13T21:57:09.528522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Discontinued this function\n",
    "def get_text(job_url):\n",
    "    '''\n",
    "    Returns all text from a job posting page. \n",
    "    '''\n",
    "    job_page = requests.get(job_urls[0])\n",
    "    job_soup = BeautifulSoup(job_page.content)\n",
    "\n",
    "    for script in job_soup(['script', 'style']):\n",
    "        script.extract()\n",
    "        \n",
    "    job_text = job_soup.get_text()\n",
    "    return job_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:09.936494Z",
     "start_time": "2019-11-13T21:57:09.931022Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for getting a job title from one url\n",
    "def get_job_title(url):\n",
    "    '''\n",
    "    Extracts a job title from a job posting page.\n",
    "    '''\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    job_title = soup.find('h3', {'class':'icl-u-xs-mb--xs icl-u-xs-mt--none jobsearch-JobInfoHeader-title'}).text\n",
    "    \n",
    "    return job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:10.309828Z",
     "start_time": "2019-11-13T21:57:10.303301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for getting a company name from one url\n",
    "def get_company_name(url):\n",
    "    '''\n",
    "    Extracts a hiring company name from a job posting page.\n",
    "    '''\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    company_name = soup.find('div', {'class':'icl-u-lg-mr--sm icl-u-xs-mr--xs'}).text\n",
    "    \n",
    "    return company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:10.778308Z",
     "start_time": "2019-11-13T21:57:10.772044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for getting a salary from one url\n",
    "def get_salary(url):\n",
    "    '''\n",
    "    Extracts a salary from a job posting page.\n",
    "    '''    \n",
    "    company_location = []\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    salary = soup.find('span', {'class':'icl-u-xs-mr--xs'}).text\n",
    "#     salary = re.sub('[^0-9]', '', salary)\n",
    "    \n",
    "    return salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:11.471165Z",
     "start_time": "2019-11-13T21:57:11.459653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for getting a company location from one url\n",
    "def get_company_location(url):\n",
    "    '''\n",
    "    Extracts a hiring company location from a job posting page.\n",
    "    '''\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    location = soup.find('title').text.split('-')[1]\n",
    "    \n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:12.533941Z",
     "start_time": "2019-11-13T21:57:12.528197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function for getting a job description from one url\n",
    "def get_job_description(url):\n",
    "    '''\n",
    "    Extracts a job discription from a job posting page.\n",
    "    '''\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    description = soup.find('div', {'class':'jobsearch-jobDescriptionText'}).text\n",
    "    \n",
    "    return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:13.219827Z",
     "start_time": "2019-11-13T21:57:13.206472Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scraping function for job posting urls:\n",
    "def scrape_urls(web_site, city, job_title_list, state):\n",
    "    '''\n",
    "    Collects urls for job postings.\n",
    "    \n",
    "    Arguments: \n",
    "        web_site = job board/search engine website\n",
    "        city = state to perform the job search for\n",
    "        job_title_list = list of position titles\n",
    "        state = state to perform the job search for\n",
    "    Outputs: \n",
    "        list of job posting urls\n",
    "    '''\n",
    "    job_url_list = []\n",
    "    for title in job_title_list:\n",
    "        \n",
    "        # Starting page for scraping:\n",
    "        start_page = os.path.join(web_site, \n",
    "                          'jobs?q=%22{}%22&l={}%2C+{}&radius=50&sort=date'.format(title, city, state))\n",
    "\n",
    "        # Get urls for pages:\n",
    "        pages_urls = get_pages_urls(start_page)\n",
    "        \n",
    "        # Get urls for jobs:\n",
    "        all_job_urls = []\n",
    "        count = 1\n",
    "        \n",
    "        for page_url in pages_urls:\n",
    "            print('collecting for {} from page {} of {} pages'.format(title, count, len(pages_urls)))\n",
    "            job_urls  = get_job_urls(page_url, web_site)\n",
    "            all_job_urls.append(job_urls)\n",
    "            count += 1\n",
    "                \n",
    "        all_job_urls = [url for sublist in all_job_urls for url in sublist] # flatten the list\n",
    "                \n",
    "        job_url_list.append(all_job_urls) # append urls from all titles we searched for\n",
    "\n",
    "              \n",
    "    all_job_urls = [url for sublist in job_url_list for url in sublist] # flatten the list\n",
    "\n",
    "    # Check for unique job urls:\n",
    "    place_job_urls = set(all_job_urls)\n",
    "    print('Number of unique urls: {}, number of all urls collected: {}'.format(len(place_job_urls), \n",
    "                                                                               len(all_job_urls)))\n",
    "    \n",
    "    return place_job_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T21:57:14.027796Z",
     "start_time": "2019-11-13T21:57:14.013048Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collects data from job posting urls:\n",
    "def collect_data(urls, file_name):\n",
    "    '''\n",
    "    Collects following job posting data - description, title, company name, company location, salary if posted.\n",
    "    Arguments:\n",
    "        urls\n",
    "    Output:\n",
    "        Dataframe with job postings as rows and collected data as features.\n",
    "    '''\n",
    "    data_dict = {}\n",
    "    data = pd.DataFrame()\n",
    "    count = 0\n",
    "    \n",
    "    folder = '/Users/greenapple/project4/data/interim/'\n",
    "#   file_name = str(file_name)\n",
    "    save_location = os.path.join(folder, '{}.pkl'.format(file_name))  \n",
    "    \n",
    "    for url in urls:\n",
    "        \n",
    "        try:\n",
    "            description = get_job_description(url) # Job description\n",
    "            data_dict['description'] = description\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            job_title = get_job_title(url) # Job title\n",
    "            data_dict['job_title'] = job_title\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            company_name = get_company_name(url)\n",
    "            data_dict['company_name'] = company_name\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            location = get_company_location(url)\n",
    "            data_dict['location'] = location\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            salary = get_salary(url)\n",
    "            data_dict['salary'] = salary\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        data_dict['link'] = url\n",
    "    \n",
    "        data = data.append(data_dict, ignore_index=True)\n",
    "        \n",
    "        # pickle dataframe   \n",
    "        pickle.dump(data, open(save_location, 'wb'))\n",
    "        \n",
    "        # Print update\n",
    "        count+=1\n",
    "        print('data point {}'.format(count))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect urls for WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T00:16:59.339245Z",
     "start_time": "2019-11-06T00:06:04.446935Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect job urls for Seattle\n",
    "\n",
    "# Scraping parameters:\n",
    "web_site = 'https://www.indeed.com/'\n",
    "city = 'Seattle'\n",
    "job_title_list = ['data+scientist',\n",
    "                  'data+analyst']\n",
    "state = 'wa'\n",
    "\n",
    "seattle_urls = scrape_urls(web_site, city, job_title_list, state)\n",
    "\n",
    "# Pickle url list:\n",
    "pickling_in = open('seattle_urls_110519.pkl', 'wb')\n",
    "pickle.dump(seattle_urls, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T00:16:59.348139Z",
     "start_time": "2019-11-06T00:16:59.341137Z"
    }
   },
   "outputs": [],
   "source": [
    "len(seattle_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T00:27:36.774048Z",
     "start_time": "2019-11-06T00:20:24.326558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect job urls for Bellevue:\n",
    "\n",
    "# Scraping parameters:\n",
    "web_site = 'https://www.indeed.com/'\n",
    "city = 'Bellevue'\n",
    "job_title_list = ['data+scientist',\n",
    "                  'data+analyst']\n",
    "state = 'wa'\n",
    "\n",
    "bellevue_urls = scrape_urls(web_site, city, job_title_list, state)\n",
    "\n",
    "# Pickle url list:\n",
    "pickling_in = open('bellevue_urls_110519.pkl', 'wb')\n",
    "pickle.dump(bellevue_urls, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T00:27:36.796227Z",
     "start_time": "2019-11-06T00:27:36.779510Z"
    }
   },
   "outputs": [],
   "source": [
    "len(bellevue_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T00:48:27.490708Z",
     "start_time": "2019-11-06T00:41:41.609432Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect non data scientist/analyst job urls for Seattle\n",
    "\n",
    "# Scraping parameters:\n",
    "web_site = 'https://www.indeed.com/'\n",
    "city = 'Seattle'\n",
    "job_title_list = ['clinical+trial',\n",
    "                  'civil+engineer']\n",
    "state = 'wa'\n",
    "\n",
    "seattle_urls_neg = scrape_urls(web_site, city, job_title_list, state)\n",
    "\n",
    "# Pickle url list:\n",
    "pickling_in = open('seattle_urls_neg_110519.pkl', 'wb')\n",
    "pickle.dump(seattle_urls_neg, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:00:43.327048Z",
     "start_time": "2019-11-13T21:57:23.899207Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for clinical+trial from page 1 of 300 pages\n",
      "collecting for clinical+trial from page 2 of 300 pages\n",
      "collecting for clinical+trial from page 3 of 300 pages\n",
      "collecting for clinical+trial from page 4 of 300 pages\n",
      "collecting for clinical+trial from page 5 of 300 pages\n",
      "collecting for clinical+trial from page 6 of 300 pages\n",
      "collecting for clinical+trial from page 7 of 300 pages\n",
      "collecting for clinical+trial from page 8 of 300 pages\n",
      "collecting for clinical+trial from page 9 of 300 pages\n",
      "collecting for clinical+trial from page 10 of 300 pages\n",
      "collecting for clinical+trial from page 11 of 300 pages\n",
      "collecting for clinical+trial from page 12 of 300 pages\n",
      "collecting for clinical+trial from page 13 of 300 pages\n",
      "collecting for clinical+trial from page 14 of 300 pages\n",
      "collecting for clinical+trial from page 15 of 300 pages\n",
      "collecting for clinical+trial from page 16 of 300 pages\n",
      "collecting for clinical+trial from page 17 of 300 pages\n",
      "collecting for clinical+trial from page 18 of 300 pages\n",
      "collecting for clinical+trial from page 19 of 300 pages\n",
      "collecting for clinical+trial from page 20 of 300 pages\n",
      "collecting for clinical+trial from page 21 of 300 pages\n",
      "collecting for clinical+trial from page 22 of 300 pages\n",
      "collecting for clinical+trial from page 23 of 300 pages\n",
      "collecting for clinical+trial from page 24 of 300 pages\n",
      "collecting for clinical+trial from page 25 of 300 pages\n",
      "collecting for clinical+trial from page 26 of 300 pages\n",
      "collecting for clinical+trial from page 27 of 300 pages\n",
      "collecting for clinical+trial from page 28 of 300 pages\n",
      "collecting for clinical+trial from page 29 of 300 pages\n",
      "collecting for clinical+trial from page 30 of 300 pages\n",
      "collecting for clinical+trial from page 31 of 300 pages\n",
      "collecting for clinical+trial from page 32 of 300 pages\n",
      "collecting for clinical+trial from page 33 of 300 pages\n",
      "collecting for clinical+trial from page 34 of 300 pages\n",
      "collecting for clinical+trial from page 35 of 300 pages\n",
      "collecting for clinical+trial from page 36 of 300 pages\n",
      "collecting for clinical+trial from page 37 of 300 pages\n",
      "collecting for clinical+trial from page 38 of 300 pages\n",
      "collecting for clinical+trial from page 39 of 300 pages\n",
      "collecting for clinical+trial from page 40 of 300 pages\n",
      "collecting for clinical+trial from page 41 of 300 pages\n",
      "collecting for clinical+trial from page 42 of 300 pages\n",
      "collecting for clinical+trial from page 43 of 300 pages\n",
      "collecting for clinical+trial from page 44 of 300 pages\n",
      "collecting for clinical+trial from page 45 of 300 pages\n",
      "collecting for clinical+trial from page 46 of 300 pages\n",
      "collecting for clinical+trial from page 47 of 300 pages\n",
      "collecting for clinical+trial from page 48 of 300 pages\n",
      "collecting for clinical+trial from page 49 of 300 pages\n",
      "collecting for clinical+trial from page 50 of 300 pages\n",
      "collecting for clinical+trial from page 51 of 300 pages\n",
      "collecting for clinical+trial from page 52 of 300 pages\n",
      "collecting for clinical+trial from page 53 of 300 pages\n",
      "collecting for clinical+trial from page 54 of 300 pages\n",
      "collecting for clinical+trial from page 55 of 300 pages\n",
      "collecting for clinical+trial from page 56 of 300 pages\n",
      "collecting for clinical+trial from page 57 of 300 pages\n",
      "collecting for clinical+trial from page 58 of 300 pages\n",
      "collecting for clinical+trial from page 59 of 300 pages\n",
      "collecting for clinical+trial from page 60 of 300 pages\n",
      "collecting for clinical+trial from page 61 of 300 pages\n",
      "collecting for clinical+trial from page 62 of 300 pages\n",
      "collecting for clinical+trial from page 63 of 300 pages\n",
      "collecting for clinical+trial from page 64 of 300 pages\n",
      "collecting for clinical+trial from page 65 of 300 pages\n",
      "collecting for clinical+trial from page 66 of 300 pages\n",
      "collecting for clinical+trial from page 67 of 300 pages\n",
      "collecting for clinical+trial from page 68 of 300 pages\n",
      "collecting for clinical+trial from page 69 of 300 pages\n",
      "collecting for clinical+trial from page 70 of 300 pages\n",
      "collecting for clinical+trial from page 71 of 300 pages\n",
      "collecting for clinical+trial from page 72 of 300 pages\n",
      "collecting for clinical+trial from page 73 of 300 pages\n",
      "collecting for clinical+trial from page 74 of 300 pages\n",
      "collecting for clinical+trial from page 75 of 300 pages\n",
      "collecting for clinical+trial from page 76 of 300 pages\n",
      "collecting for clinical+trial from page 77 of 300 pages\n",
      "collecting for clinical+trial from page 78 of 300 pages\n",
      "collecting for clinical+trial from page 79 of 300 pages\n",
      "collecting for clinical+trial from page 80 of 300 pages\n",
      "collecting for clinical+trial from page 81 of 300 pages\n",
      "collecting for clinical+trial from page 82 of 300 pages\n",
      "collecting for clinical+trial from page 83 of 300 pages\n",
      "collecting for clinical+trial from page 84 of 300 pages\n",
      "collecting for clinical+trial from page 85 of 300 pages\n",
      "collecting for clinical+trial from page 86 of 300 pages\n",
      "collecting for clinical+trial from page 87 of 300 pages\n",
      "collecting for clinical+trial from page 88 of 300 pages\n",
      "collecting for clinical+trial from page 89 of 300 pages\n",
      "collecting for clinical+trial from page 90 of 300 pages\n",
      "collecting for clinical+trial from page 91 of 300 pages\n",
      "collecting for clinical+trial from page 92 of 300 pages\n",
      "collecting for clinical+trial from page 93 of 300 pages\n",
      "collecting for clinical+trial from page 94 of 300 pages\n",
      "collecting for clinical+trial from page 95 of 300 pages\n",
      "collecting for clinical+trial from page 96 of 300 pages\n",
      "collecting for clinical+trial from page 97 of 300 pages\n",
      "collecting for clinical+trial from page 98 of 300 pages\n",
      "collecting for clinical+trial from page 99 of 300 pages\n",
      "collecting for clinical+trial from page 100 of 300 pages\n",
      "collecting for clinical+trial from page 101 of 300 pages\n",
      "collecting for clinical+trial from page 102 of 300 pages\n",
      "collecting for clinical+trial from page 103 of 300 pages\n",
      "collecting for clinical+trial from page 104 of 300 pages\n",
      "collecting for clinical+trial from page 105 of 300 pages\n",
      "collecting for clinical+trial from page 106 of 300 pages\n",
      "collecting for clinical+trial from page 107 of 300 pages\n",
      "collecting for clinical+trial from page 108 of 300 pages\n",
      "collecting for clinical+trial from page 109 of 300 pages\n",
      "collecting for clinical+trial from page 110 of 300 pages\n",
      "collecting for clinical+trial from page 111 of 300 pages\n",
      "collecting for clinical+trial from page 112 of 300 pages\n",
      "collecting for clinical+trial from page 113 of 300 pages\n",
      "collecting for clinical+trial from page 114 of 300 pages\n",
      "collecting for clinical+trial from page 115 of 300 pages\n",
      "collecting for clinical+trial from page 116 of 300 pages\n",
      "collecting for clinical+trial from page 117 of 300 pages\n",
      "collecting for clinical+trial from page 118 of 300 pages\n",
      "collecting for clinical+trial from page 119 of 300 pages\n",
      "collecting for clinical+trial from page 120 of 300 pages\n",
      "collecting for clinical+trial from page 121 of 300 pages\n",
      "collecting for clinical+trial from page 122 of 300 pages\n",
      "collecting for clinical+trial from page 123 of 300 pages\n",
      "collecting for clinical+trial from page 124 of 300 pages\n",
      "collecting for clinical+trial from page 125 of 300 pages\n",
      "collecting for clinical+trial from page 126 of 300 pages\n",
      "collecting for clinical+trial from page 127 of 300 pages\n",
      "collecting for clinical+trial from page 128 of 300 pages\n",
      "collecting for clinical+trial from page 129 of 300 pages\n",
      "collecting for clinical+trial from page 130 of 300 pages\n",
      "collecting for clinical+trial from page 131 of 300 pages\n",
      "collecting for clinical+trial from page 132 of 300 pages\n",
      "collecting for clinical+trial from page 133 of 300 pages\n",
      "collecting for clinical+trial from page 134 of 300 pages\n",
      "collecting for clinical+trial from page 135 of 300 pages\n",
      "collecting for clinical+trial from page 136 of 300 pages\n",
      "collecting for clinical+trial from page 137 of 300 pages\n",
      "collecting for clinical+trial from page 138 of 300 pages\n",
      "collecting for clinical+trial from page 139 of 300 pages\n",
      "collecting for clinical+trial from page 140 of 300 pages\n",
      "collecting for clinical+trial from page 141 of 300 pages\n",
      "collecting for clinical+trial from page 142 of 300 pages\n",
      "collecting for clinical+trial from page 143 of 300 pages\n",
      "collecting for clinical+trial from page 144 of 300 pages\n",
      "collecting for clinical+trial from page 145 of 300 pages\n",
      "collecting for clinical+trial from page 146 of 300 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for clinical+trial from page 147 of 300 pages\n",
      "collecting for clinical+trial from page 148 of 300 pages\n",
      "collecting for clinical+trial from page 149 of 300 pages\n",
      "collecting for clinical+trial from page 150 of 300 pages\n",
      "collecting for clinical+trial from page 151 of 300 pages\n",
      "collecting for clinical+trial from page 152 of 300 pages\n",
      "collecting for clinical+trial from page 153 of 300 pages\n",
      "collecting for clinical+trial from page 154 of 300 pages\n",
      "collecting for clinical+trial from page 155 of 300 pages\n",
      "collecting for clinical+trial from page 156 of 300 pages\n",
      "collecting for clinical+trial from page 157 of 300 pages\n",
      "collecting for clinical+trial from page 158 of 300 pages\n",
      "collecting for clinical+trial from page 159 of 300 pages\n",
      "collecting for clinical+trial from page 160 of 300 pages\n",
      "collecting for clinical+trial from page 161 of 300 pages\n",
      "collecting for clinical+trial from page 162 of 300 pages\n",
      "collecting for clinical+trial from page 163 of 300 pages\n",
      "collecting for clinical+trial from page 164 of 300 pages\n",
      "collecting for clinical+trial from page 165 of 300 pages\n",
      "collecting for clinical+trial from page 166 of 300 pages\n",
      "collecting for clinical+trial from page 167 of 300 pages\n",
      "collecting for clinical+trial from page 168 of 300 pages\n",
      "collecting for clinical+trial from page 169 of 300 pages\n",
      "collecting for clinical+trial from page 170 of 300 pages\n",
      "collecting for clinical+trial from page 171 of 300 pages\n",
      "collecting for clinical+trial from page 172 of 300 pages\n",
      "collecting for clinical+trial from page 173 of 300 pages\n",
      "collecting for clinical+trial from page 174 of 300 pages\n",
      "collecting for clinical+trial from page 175 of 300 pages\n",
      "collecting for clinical+trial from page 176 of 300 pages\n",
      "collecting for clinical+trial from page 177 of 300 pages\n",
      "collecting for clinical+trial from page 178 of 300 pages\n",
      "collecting for clinical+trial from page 179 of 300 pages\n",
      "collecting for clinical+trial from page 180 of 300 pages\n",
      "collecting for clinical+trial from page 181 of 300 pages\n",
      "collecting for clinical+trial from page 182 of 300 pages\n",
      "collecting for clinical+trial from page 183 of 300 pages\n",
      "collecting for clinical+trial from page 184 of 300 pages\n",
      "collecting for clinical+trial from page 185 of 300 pages\n",
      "collecting for clinical+trial from page 186 of 300 pages\n",
      "collecting for clinical+trial from page 187 of 300 pages\n",
      "collecting for clinical+trial from page 188 of 300 pages\n",
      "collecting for clinical+trial from page 189 of 300 pages\n",
      "collecting for clinical+trial from page 190 of 300 pages\n",
      "collecting for clinical+trial from page 191 of 300 pages\n",
      "collecting for clinical+trial from page 192 of 300 pages\n",
      "collecting for clinical+trial from page 193 of 300 pages\n",
      "collecting for clinical+trial from page 194 of 300 pages\n",
      "collecting for clinical+trial from page 195 of 300 pages\n",
      "collecting for clinical+trial from page 196 of 300 pages\n",
      "collecting for clinical+trial from page 197 of 300 pages\n",
      "collecting for clinical+trial from page 198 of 300 pages\n",
      "collecting for clinical+trial from page 199 of 300 pages\n",
      "collecting for clinical+trial from page 200 of 300 pages\n",
      "collecting for clinical+trial from page 201 of 300 pages\n",
      "collecting for clinical+trial from page 202 of 300 pages\n",
      "collecting for clinical+trial from page 203 of 300 pages\n",
      "collecting for clinical+trial from page 204 of 300 pages\n",
      "collecting for clinical+trial from page 205 of 300 pages\n",
      "collecting for clinical+trial from page 206 of 300 pages\n",
      "collecting for clinical+trial from page 207 of 300 pages\n",
      "collecting for clinical+trial from page 208 of 300 pages\n",
      "collecting for clinical+trial from page 209 of 300 pages\n",
      "collecting for clinical+trial from page 210 of 300 pages\n",
      "collecting for clinical+trial from page 211 of 300 pages\n",
      "collecting for clinical+trial from page 212 of 300 pages\n",
      "collecting for clinical+trial from page 213 of 300 pages\n",
      "collecting for clinical+trial from page 214 of 300 pages\n",
      "collecting for clinical+trial from page 215 of 300 pages\n",
      "collecting for clinical+trial from page 216 of 300 pages\n",
      "collecting for clinical+trial from page 217 of 300 pages\n",
      "collecting for clinical+trial from page 218 of 300 pages\n",
      "collecting for clinical+trial from page 219 of 300 pages\n",
      "collecting for clinical+trial from page 220 of 300 pages\n",
      "collecting for clinical+trial from page 221 of 300 pages\n",
      "collecting for clinical+trial from page 222 of 300 pages\n",
      "collecting for clinical+trial from page 223 of 300 pages\n",
      "collecting for clinical+trial from page 224 of 300 pages\n",
      "collecting for clinical+trial from page 225 of 300 pages\n",
      "collecting for clinical+trial from page 226 of 300 pages\n",
      "collecting for clinical+trial from page 227 of 300 pages\n",
      "collecting for clinical+trial from page 228 of 300 pages\n",
      "collecting for clinical+trial from page 229 of 300 pages\n",
      "collecting for clinical+trial from page 230 of 300 pages\n",
      "collecting for clinical+trial from page 231 of 300 pages\n",
      "collecting for clinical+trial from page 232 of 300 pages\n",
      "collecting for clinical+trial from page 233 of 300 pages\n",
      "collecting for clinical+trial from page 234 of 300 pages\n",
      "collecting for clinical+trial from page 235 of 300 pages\n",
      "collecting for clinical+trial from page 236 of 300 pages\n",
      "collecting for clinical+trial from page 237 of 300 pages\n",
      "collecting for clinical+trial from page 238 of 300 pages\n",
      "collecting for clinical+trial from page 239 of 300 pages\n",
      "collecting for clinical+trial from page 240 of 300 pages\n",
      "collecting for clinical+trial from page 241 of 300 pages\n",
      "collecting for clinical+trial from page 242 of 300 pages\n",
      "collecting for clinical+trial from page 243 of 300 pages\n",
      "collecting for clinical+trial from page 244 of 300 pages\n",
      "collecting for clinical+trial from page 245 of 300 pages\n",
      "collecting for clinical+trial from page 246 of 300 pages\n",
      "collecting for clinical+trial from page 247 of 300 pages\n",
      "collecting for clinical+trial from page 248 of 300 pages\n",
      "collecting for clinical+trial from page 249 of 300 pages\n",
      "collecting for clinical+trial from page 250 of 300 pages\n",
      "collecting for clinical+trial from page 251 of 300 pages\n",
      "collecting for clinical+trial from page 252 of 300 pages\n",
      "collecting for clinical+trial from page 253 of 300 pages\n",
      "collecting for clinical+trial from page 254 of 300 pages\n",
      "collecting for clinical+trial from page 255 of 300 pages\n",
      "collecting for clinical+trial from page 256 of 300 pages\n",
      "collecting for clinical+trial from page 257 of 300 pages\n",
      "collecting for clinical+trial from page 258 of 300 pages\n",
      "collecting for clinical+trial from page 259 of 300 pages\n",
      "collecting for clinical+trial from page 260 of 300 pages\n",
      "collecting for clinical+trial from page 261 of 300 pages\n",
      "collecting for clinical+trial from page 262 of 300 pages\n",
      "collecting for clinical+trial from page 263 of 300 pages\n",
      "collecting for clinical+trial from page 264 of 300 pages\n",
      "collecting for clinical+trial from page 265 of 300 pages\n",
      "collecting for clinical+trial from page 266 of 300 pages\n",
      "collecting for clinical+trial from page 267 of 300 pages\n",
      "collecting for clinical+trial from page 268 of 300 pages\n",
      "collecting for clinical+trial from page 269 of 300 pages\n",
      "collecting for clinical+trial from page 270 of 300 pages\n",
      "collecting for clinical+trial from page 271 of 300 pages\n",
      "collecting for clinical+trial from page 272 of 300 pages\n",
      "collecting for clinical+trial from page 273 of 300 pages\n",
      "collecting for clinical+trial from page 274 of 300 pages\n",
      "collecting for clinical+trial from page 275 of 300 pages\n",
      "collecting for clinical+trial from page 276 of 300 pages\n",
      "collecting for clinical+trial from page 277 of 300 pages\n",
      "collecting for clinical+trial from page 278 of 300 pages\n",
      "collecting for clinical+trial from page 279 of 300 pages\n",
      "collecting for clinical+trial from page 280 of 300 pages\n",
      "collecting for clinical+trial from page 281 of 300 pages\n",
      "collecting for clinical+trial from page 282 of 300 pages\n",
      "collecting for clinical+trial from page 283 of 300 pages\n",
      "collecting for clinical+trial from page 284 of 300 pages\n",
      "collecting for clinical+trial from page 285 of 300 pages\n",
      "collecting for clinical+trial from page 286 of 300 pages\n",
      "collecting for clinical+trial from page 287 of 300 pages\n",
      "collecting for clinical+trial from page 288 of 300 pages\n",
      "collecting for clinical+trial from page 289 of 300 pages\n",
      "collecting for clinical+trial from page 290 of 300 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for clinical+trial from page 291 of 300 pages\n",
      "collecting for clinical+trial from page 292 of 300 pages\n",
      "collecting for clinical+trial from page 293 of 300 pages\n",
      "collecting for clinical+trial from page 294 of 300 pages\n",
      "collecting for clinical+trial from page 295 of 300 pages\n",
      "collecting for clinical+trial from page 296 of 300 pages\n",
      "collecting for clinical+trial from page 297 of 300 pages\n",
      "collecting for clinical+trial from page 298 of 300 pages\n",
      "collecting for clinical+trial from page 299 of 300 pages\n",
      "collecting for clinical+trial from page 300 of 300 pages\n",
      "Number of unique urls: 1655, number of all urls collected: 4497\n"
     ]
    }
   ],
   "source": [
    "# Collect clinical+trial job urls for Seattle\n",
    "\n",
    "# Scraping parameters:\n",
    "web_site = 'https://www.indeed.com/'\n",
    "city = 'Seattle'\n",
    "job_title_list = ['clinical+trial']\n",
    "state = 'wa'\n",
    "\n",
    "seattle_urls_neg = scrape_urls(web_site, city, job_title_list, state)\n",
    "\n",
    "# Pickle url list:\n",
    "pickling_in = open('seattle_urls_clinical_trial_111319.pkl', 'wb')\n",
    "pickle.dump(seattle_urls_neg, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-13T22:05:40.207967Z",
     "start_time": "2019-11-13T22:01:40.514345Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for civil+engineer from page 1 of 300 pages\n",
      "collecting for civil+engineer from page 2 of 300 pages\n",
      "collecting for civil+engineer from page 3 of 300 pages\n",
      "collecting for civil+engineer from page 4 of 300 pages\n",
      "collecting for civil+engineer from page 5 of 300 pages\n",
      "collecting for civil+engineer from page 6 of 300 pages\n",
      "collecting for civil+engineer from page 7 of 300 pages\n",
      "collecting for civil+engineer from page 8 of 300 pages\n",
      "collecting for civil+engineer from page 9 of 300 pages\n",
      "collecting for civil+engineer from page 10 of 300 pages\n",
      "collecting for civil+engineer from page 11 of 300 pages\n",
      "collecting for civil+engineer from page 12 of 300 pages\n",
      "collecting for civil+engineer from page 13 of 300 pages\n",
      "collecting for civil+engineer from page 14 of 300 pages\n",
      "collecting for civil+engineer from page 15 of 300 pages\n",
      "collecting for civil+engineer from page 16 of 300 pages\n",
      "collecting for civil+engineer from page 17 of 300 pages\n",
      "collecting for civil+engineer from page 18 of 300 pages\n",
      "collecting for civil+engineer from page 19 of 300 pages\n",
      "collecting for civil+engineer from page 20 of 300 pages\n",
      "collecting for civil+engineer from page 21 of 300 pages\n",
      "collecting for civil+engineer from page 22 of 300 pages\n",
      "collecting for civil+engineer from page 23 of 300 pages\n",
      "collecting for civil+engineer from page 24 of 300 pages\n",
      "collecting for civil+engineer from page 25 of 300 pages\n",
      "collecting for civil+engineer from page 26 of 300 pages\n",
      "collecting for civil+engineer from page 27 of 300 pages\n",
      "collecting for civil+engineer from page 28 of 300 pages\n",
      "collecting for civil+engineer from page 29 of 300 pages\n",
      "collecting for civil+engineer from page 30 of 300 pages\n",
      "collecting for civil+engineer from page 31 of 300 pages\n",
      "collecting for civil+engineer from page 32 of 300 pages\n",
      "collecting for civil+engineer from page 33 of 300 pages\n",
      "collecting for civil+engineer from page 34 of 300 pages\n",
      "collecting for civil+engineer from page 35 of 300 pages\n",
      "collecting for civil+engineer from page 36 of 300 pages\n",
      "collecting for civil+engineer from page 37 of 300 pages\n",
      "collecting for civil+engineer from page 38 of 300 pages\n",
      "collecting for civil+engineer from page 39 of 300 pages\n",
      "collecting for civil+engineer from page 40 of 300 pages\n",
      "collecting for civil+engineer from page 41 of 300 pages\n",
      "collecting for civil+engineer from page 42 of 300 pages\n",
      "collecting for civil+engineer from page 43 of 300 pages\n",
      "collecting for civil+engineer from page 44 of 300 pages\n",
      "collecting for civil+engineer from page 45 of 300 pages\n",
      "collecting for civil+engineer from page 46 of 300 pages\n",
      "collecting for civil+engineer from page 47 of 300 pages\n",
      "collecting for civil+engineer from page 48 of 300 pages\n",
      "collecting for civil+engineer from page 49 of 300 pages\n",
      "collecting for civil+engineer from page 50 of 300 pages\n",
      "collecting for civil+engineer from page 51 of 300 pages\n",
      "collecting for civil+engineer from page 52 of 300 pages\n",
      "collecting for civil+engineer from page 53 of 300 pages\n",
      "collecting for civil+engineer from page 54 of 300 pages\n",
      "collecting for civil+engineer from page 55 of 300 pages\n",
      "collecting for civil+engineer from page 56 of 300 pages\n",
      "collecting for civil+engineer from page 57 of 300 pages\n",
      "collecting for civil+engineer from page 58 of 300 pages\n",
      "collecting for civil+engineer from page 59 of 300 pages\n",
      "collecting for civil+engineer from page 60 of 300 pages\n",
      "collecting for civil+engineer from page 61 of 300 pages\n",
      "collecting for civil+engineer from page 62 of 300 pages\n",
      "collecting for civil+engineer from page 63 of 300 pages\n",
      "collecting for civil+engineer from page 64 of 300 pages\n",
      "collecting for civil+engineer from page 65 of 300 pages\n",
      "collecting for civil+engineer from page 66 of 300 pages\n",
      "collecting for civil+engineer from page 67 of 300 pages\n",
      "collecting for civil+engineer from page 68 of 300 pages\n",
      "collecting for civil+engineer from page 69 of 300 pages\n",
      "collecting for civil+engineer from page 70 of 300 pages\n",
      "collecting for civil+engineer from page 71 of 300 pages\n",
      "collecting for civil+engineer from page 72 of 300 pages\n",
      "collecting for civil+engineer from page 73 of 300 pages\n",
      "collecting for civil+engineer from page 74 of 300 pages\n",
      "collecting for civil+engineer from page 75 of 300 pages\n",
      "collecting for civil+engineer from page 76 of 300 pages\n",
      "collecting for civil+engineer from page 77 of 300 pages\n",
      "collecting for civil+engineer from page 78 of 300 pages\n",
      "collecting for civil+engineer from page 79 of 300 pages\n",
      "collecting for civil+engineer from page 80 of 300 pages\n",
      "collecting for civil+engineer from page 81 of 300 pages\n",
      "collecting for civil+engineer from page 82 of 300 pages\n",
      "collecting for civil+engineer from page 83 of 300 pages\n",
      "collecting for civil+engineer from page 84 of 300 pages\n",
      "collecting for civil+engineer from page 85 of 300 pages\n",
      "collecting for civil+engineer from page 86 of 300 pages\n",
      "collecting for civil+engineer from page 87 of 300 pages\n",
      "collecting for civil+engineer from page 88 of 300 pages\n",
      "collecting for civil+engineer from page 89 of 300 pages\n",
      "collecting for civil+engineer from page 90 of 300 pages\n",
      "collecting for civil+engineer from page 91 of 300 pages\n",
      "collecting for civil+engineer from page 92 of 300 pages\n",
      "collecting for civil+engineer from page 93 of 300 pages\n",
      "collecting for civil+engineer from page 94 of 300 pages\n",
      "collecting for civil+engineer from page 95 of 300 pages\n",
      "collecting for civil+engineer from page 96 of 300 pages\n",
      "collecting for civil+engineer from page 97 of 300 pages\n",
      "collecting for civil+engineer from page 98 of 300 pages\n",
      "collecting for civil+engineer from page 99 of 300 pages\n",
      "collecting for civil+engineer from page 100 of 300 pages\n",
      "collecting for civil+engineer from page 101 of 300 pages\n",
      "collecting for civil+engineer from page 102 of 300 pages\n",
      "collecting for civil+engineer from page 103 of 300 pages\n",
      "collecting for civil+engineer from page 104 of 300 pages\n",
      "collecting for civil+engineer from page 105 of 300 pages\n",
      "collecting for civil+engineer from page 106 of 300 pages\n",
      "collecting for civil+engineer from page 107 of 300 pages\n",
      "collecting for civil+engineer from page 108 of 300 pages\n",
      "collecting for civil+engineer from page 109 of 300 pages\n",
      "collecting for civil+engineer from page 110 of 300 pages\n",
      "collecting for civil+engineer from page 111 of 300 pages\n",
      "collecting for civil+engineer from page 112 of 300 pages\n",
      "collecting for civil+engineer from page 113 of 300 pages\n",
      "collecting for civil+engineer from page 114 of 300 pages\n",
      "collecting for civil+engineer from page 115 of 300 pages\n",
      "collecting for civil+engineer from page 116 of 300 pages\n",
      "collecting for civil+engineer from page 117 of 300 pages\n",
      "collecting for civil+engineer from page 118 of 300 pages\n",
      "collecting for civil+engineer from page 119 of 300 pages\n",
      "collecting for civil+engineer from page 120 of 300 pages\n",
      "collecting for civil+engineer from page 121 of 300 pages\n",
      "collecting for civil+engineer from page 122 of 300 pages\n",
      "collecting for civil+engineer from page 123 of 300 pages\n",
      "collecting for civil+engineer from page 124 of 300 pages\n",
      "collecting for civil+engineer from page 125 of 300 pages\n",
      "collecting for civil+engineer from page 126 of 300 pages\n",
      "collecting for civil+engineer from page 127 of 300 pages\n",
      "collecting for civil+engineer from page 128 of 300 pages\n",
      "collecting for civil+engineer from page 129 of 300 pages\n",
      "collecting for civil+engineer from page 130 of 300 pages\n",
      "collecting for civil+engineer from page 131 of 300 pages\n",
      "collecting for civil+engineer from page 132 of 300 pages\n",
      "collecting for civil+engineer from page 133 of 300 pages\n",
      "collecting for civil+engineer from page 134 of 300 pages\n",
      "collecting for civil+engineer from page 135 of 300 pages\n",
      "collecting for civil+engineer from page 136 of 300 pages\n",
      "collecting for civil+engineer from page 137 of 300 pages\n",
      "collecting for civil+engineer from page 138 of 300 pages\n",
      "collecting for civil+engineer from page 139 of 300 pages\n",
      "collecting for civil+engineer from page 140 of 300 pages\n",
      "collecting for civil+engineer from page 141 of 300 pages\n",
      "collecting for civil+engineer from page 142 of 300 pages\n",
      "collecting for civil+engineer from page 143 of 300 pages\n",
      "collecting for civil+engineer from page 144 of 300 pages\n",
      "collecting for civil+engineer from page 145 of 300 pages\n",
      "collecting for civil+engineer from page 146 of 300 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for civil+engineer from page 147 of 300 pages\n",
      "collecting for civil+engineer from page 148 of 300 pages\n",
      "collecting for civil+engineer from page 149 of 300 pages\n",
      "collecting for civil+engineer from page 150 of 300 pages\n",
      "collecting for civil+engineer from page 151 of 300 pages\n",
      "collecting for civil+engineer from page 152 of 300 pages\n",
      "collecting for civil+engineer from page 153 of 300 pages\n",
      "collecting for civil+engineer from page 154 of 300 pages\n",
      "collecting for civil+engineer from page 155 of 300 pages\n",
      "collecting for civil+engineer from page 156 of 300 pages\n",
      "collecting for civil+engineer from page 157 of 300 pages\n",
      "collecting for civil+engineer from page 158 of 300 pages\n",
      "collecting for civil+engineer from page 159 of 300 pages\n",
      "collecting for civil+engineer from page 160 of 300 pages\n",
      "collecting for civil+engineer from page 161 of 300 pages\n",
      "collecting for civil+engineer from page 162 of 300 pages\n",
      "collecting for civil+engineer from page 163 of 300 pages\n",
      "collecting for civil+engineer from page 164 of 300 pages\n",
      "collecting for civil+engineer from page 165 of 300 pages\n",
      "collecting for civil+engineer from page 166 of 300 pages\n",
      "collecting for civil+engineer from page 167 of 300 pages\n",
      "collecting for civil+engineer from page 168 of 300 pages\n",
      "collecting for civil+engineer from page 169 of 300 pages\n",
      "collecting for civil+engineer from page 170 of 300 pages\n",
      "collecting for civil+engineer from page 171 of 300 pages\n",
      "collecting for civil+engineer from page 172 of 300 pages\n",
      "collecting for civil+engineer from page 173 of 300 pages\n",
      "collecting for civil+engineer from page 174 of 300 pages\n",
      "collecting for civil+engineer from page 175 of 300 pages\n",
      "collecting for civil+engineer from page 176 of 300 pages\n",
      "collecting for civil+engineer from page 177 of 300 pages\n",
      "collecting for civil+engineer from page 178 of 300 pages\n",
      "collecting for civil+engineer from page 179 of 300 pages\n",
      "collecting for civil+engineer from page 180 of 300 pages\n",
      "collecting for civil+engineer from page 181 of 300 pages\n",
      "collecting for civil+engineer from page 182 of 300 pages\n",
      "collecting for civil+engineer from page 183 of 300 pages\n",
      "collecting for civil+engineer from page 184 of 300 pages\n",
      "collecting for civil+engineer from page 185 of 300 pages\n",
      "collecting for civil+engineer from page 186 of 300 pages\n",
      "collecting for civil+engineer from page 187 of 300 pages\n",
      "collecting for civil+engineer from page 188 of 300 pages\n",
      "collecting for civil+engineer from page 189 of 300 pages\n",
      "collecting for civil+engineer from page 190 of 300 pages\n",
      "collecting for civil+engineer from page 191 of 300 pages\n",
      "collecting for civil+engineer from page 192 of 300 pages\n",
      "collecting for civil+engineer from page 193 of 300 pages\n",
      "collecting for civil+engineer from page 194 of 300 pages\n",
      "collecting for civil+engineer from page 195 of 300 pages\n",
      "collecting for civil+engineer from page 196 of 300 pages\n",
      "collecting for civil+engineer from page 197 of 300 pages\n",
      "collecting for civil+engineer from page 198 of 300 pages\n",
      "collecting for civil+engineer from page 199 of 300 pages\n",
      "collecting for civil+engineer from page 200 of 300 pages\n",
      "collecting for civil+engineer from page 201 of 300 pages\n",
      "collecting for civil+engineer from page 202 of 300 pages\n",
      "collecting for civil+engineer from page 203 of 300 pages\n",
      "collecting for civil+engineer from page 204 of 300 pages\n",
      "collecting for civil+engineer from page 205 of 300 pages\n",
      "collecting for civil+engineer from page 206 of 300 pages\n",
      "collecting for civil+engineer from page 207 of 300 pages\n",
      "collecting for civil+engineer from page 208 of 300 pages\n",
      "collecting for civil+engineer from page 209 of 300 pages\n",
      "collecting for civil+engineer from page 210 of 300 pages\n",
      "collecting for civil+engineer from page 211 of 300 pages\n",
      "collecting for civil+engineer from page 212 of 300 pages\n",
      "collecting for civil+engineer from page 213 of 300 pages\n",
      "collecting for civil+engineer from page 214 of 300 pages\n",
      "collecting for civil+engineer from page 215 of 300 pages\n",
      "collecting for civil+engineer from page 216 of 300 pages\n",
      "collecting for civil+engineer from page 217 of 300 pages\n",
      "collecting for civil+engineer from page 218 of 300 pages\n",
      "collecting for civil+engineer from page 219 of 300 pages\n",
      "collecting for civil+engineer from page 220 of 300 pages\n",
      "collecting for civil+engineer from page 221 of 300 pages\n",
      "collecting for civil+engineer from page 222 of 300 pages\n",
      "collecting for civil+engineer from page 223 of 300 pages\n",
      "collecting for civil+engineer from page 224 of 300 pages\n",
      "collecting for civil+engineer from page 225 of 300 pages\n",
      "collecting for civil+engineer from page 226 of 300 pages\n",
      "collecting for civil+engineer from page 227 of 300 pages\n",
      "collecting for civil+engineer from page 228 of 300 pages\n",
      "collecting for civil+engineer from page 229 of 300 pages\n",
      "collecting for civil+engineer from page 230 of 300 pages\n",
      "collecting for civil+engineer from page 231 of 300 pages\n",
      "collecting for civil+engineer from page 232 of 300 pages\n",
      "collecting for civil+engineer from page 233 of 300 pages\n",
      "collecting for civil+engineer from page 234 of 300 pages\n",
      "collecting for civil+engineer from page 235 of 300 pages\n",
      "collecting for civil+engineer from page 236 of 300 pages\n",
      "collecting for civil+engineer from page 237 of 300 pages\n",
      "collecting for civil+engineer from page 238 of 300 pages\n",
      "collecting for civil+engineer from page 239 of 300 pages\n",
      "collecting for civil+engineer from page 240 of 300 pages\n",
      "collecting for civil+engineer from page 241 of 300 pages\n",
      "collecting for civil+engineer from page 242 of 300 pages\n",
      "collecting for civil+engineer from page 243 of 300 pages\n",
      "collecting for civil+engineer from page 244 of 300 pages\n",
      "collecting for civil+engineer from page 245 of 300 pages\n",
      "collecting for civil+engineer from page 246 of 300 pages\n",
      "collecting for civil+engineer from page 247 of 300 pages\n",
      "collecting for civil+engineer from page 248 of 300 pages\n",
      "collecting for civil+engineer from page 249 of 300 pages\n",
      "collecting for civil+engineer from page 250 of 300 pages\n",
      "collecting for civil+engineer from page 251 of 300 pages\n",
      "collecting for civil+engineer from page 252 of 300 pages\n",
      "collecting for civil+engineer from page 253 of 300 pages\n",
      "collecting for civil+engineer from page 254 of 300 pages\n",
      "collecting for civil+engineer from page 255 of 300 pages\n",
      "collecting for civil+engineer from page 256 of 300 pages\n",
      "collecting for civil+engineer from page 257 of 300 pages\n",
      "collecting for civil+engineer from page 258 of 300 pages\n",
      "collecting for civil+engineer from page 259 of 300 pages\n",
      "collecting for civil+engineer from page 260 of 300 pages\n",
      "collecting for civil+engineer from page 261 of 300 pages\n",
      "collecting for civil+engineer from page 262 of 300 pages\n",
      "collecting for civil+engineer from page 263 of 300 pages\n",
      "collecting for civil+engineer from page 264 of 300 pages\n",
      "collecting for civil+engineer from page 265 of 300 pages\n",
      "collecting for civil+engineer from page 266 of 300 pages\n",
      "collecting for civil+engineer from page 267 of 300 pages\n",
      "collecting for civil+engineer from page 268 of 300 pages\n",
      "collecting for civil+engineer from page 269 of 300 pages\n",
      "collecting for civil+engineer from page 270 of 300 pages\n",
      "collecting for civil+engineer from page 271 of 300 pages\n",
      "collecting for civil+engineer from page 272 of 300 pages\n",
      "collecting for civil+engineer from page 273 of 300 pages\n",
      "collecting for civil+engineer from page 274 of 300 pages\n",
      "collecting for civil+engineer from page 275 of 300 pages\n",
      "collecting for civil+engineer from page 276 of 300 pages\n",
      "collecting for civil+engineer from page 277 of 300 pages\n",
      "collecting for civil+engineer from page 278 of 300 pages\n",
      "collecting for civil+engineer from page 279 of 300 pages\n",
      "collecting for civil+engineer from page 280 of 300 pages\n",
      "collecting for civil+engineer from page 281 of 300 pages\n",
      "collecting for civil+engineer from page 282 of 300 pages\n",
      "collecting for civil+engineer from page 283 of 300 pages\n",
      "collecting for civil+engineer from page 284 of 300 pages\n",
      "collecting for civil+engineer from page 285 of 300 pages\n",
      "collecting for civil+engineer from page 286 of 300 pages\n",
      "collecting for civil+engineer from page 287 of 300 pages\n",
      "collecting for civil+engineer from page 288 of 300 pages\n",
      "collecting for civil+engineer from page 289 of 300 pages\n",
      "collecting for civil+engineer from page 290 of 300 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for civil+engineer from page 291 of 300 pages\n",
      "collecting for civil+engineer from page 292 of 300 pages\n",
      "collecting for civil+engineer from page 293 of 300 pages\n",
      "collecting for civil+engineer from page 294 of 300 pages\n",
      "collecting for civil+engineer from page 295 of 300 pages\n",
      "collecting for civil+engineer from page 296 of 300 pages\n",
      "collecting for civil+engineer from page 297 of 300 pages\n",
      "collecting for civil+engineer from page 298 of 300 pages\n",
      "collecting for civil+engineer from page 299 of 300 pages\n",
      "collecting for civil+engineer from page 300 of 300 pages\n",
      "Number of unique urls: 2761, number of all urls collected: 5547\n"
     ]
    }
   ],
   "source": [
    "# Collect civil+engineer job urls for Seattle\n",
    "\n",
    "# Scraping parameters:\n",
    "web_site = 'https://www.indeed.com/'\n",
    "city = 'Seattle'\n",
    "job_title_list = ['civil+engineer']\n",
    "state = 'wa'\n",
    "\n",
    "seattle_urls_neg = scrape_urls(web_site, city, job_title_list, state)\n",
    "\n",
    "# Pickle url list:\n",
    "pickling_in = open('seattle_urls_civil_engineer_111319.pkl', 'wb')\n",
    "pickle.dump(seattle_urls_neg, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine WA urls and make sure they are unique:\n",
    "washington_job_urls = seattle_urls.union(bellevue_urls, seattle_urls_neg)\n",
    "len(washington_job_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle:\n",
    "pickling_in = open('/Users/greenapple/project4/data/interim/washington_job_urls_110519.pkl', 'wb')\n",
    "pickle.dump(washington_job_urls, pickling_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect urls for CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T07:39:48.715478Z",
     "start_time": "2019-11-11T07:33:22.477009Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for data+scientist from page 1 of 300 pages\n",
      "collecting for data+scientist from page 2 of 300 pages\n",
      "collecting for data+scientist from page 3 of 300 pages\n",
      "collecting for data+scientist from page 4 of 300 pages\n",
      "collecting for data+scientist from page 5 of 300 pages\n",
      "collecting for data+scientist from page 6 of 300 pages\n",
      "collecting for data+scientist from page 7 of 300 pages\n",
      "collecting for data+scientist from page 8 of 300 pages\n",
      "collecting for data+scientist from page 9 of 300 pages\n",
      "collecting for data+scientist from page 10 of 300 pages\n",
      "collecting for data+scientist from page 11 of 300 pages\n",
      "collecting for data+scientist from page 12 of 300 pages\n",
      "collecting for data+scientist from page 13 of 300 pages\n",
      "collecting for data+scientist from page 14 of 300 pages\n",
      "collecting for data+scientist from page 15 of 300 pages\n",
      "collecting for data+scientist from page 16 of 300 pages\n",
      "collecting for data+scientist from page 17 of 300 pages\n",
      "collecting for data+scientist from page 18 of 300 pages\n",
      "collecting for data+scientist from page 19 of 300 pages\n",
      "collecting for data+scientist from page 20 of 300 pages\n",
      "collecting for data+scientist from page 21 of 300 pages\n",
      "collecting for data+scientist from page 22 of 300 pages\n",
      "collecting for data+scientist from page 23 of 300 pages\n",
      "collecting for data+scientist from page 24 of 300 pages\n",
      "collecting for data+scientist from page 25 of 300 pages\n",
      "collecting for data+scientist from page 26 of 300 pages\n",
      "collecting for data+scientist from page 27 of 300 pages\n",
      "collecting for data+scientist from page 28 of 300 pages\n",
      "collecting for data+scientist from page 29 of 300 pages\n",
      "collecting for data+scientist from page 30 of 300 pages\n",
      "collecting for data+scientist from page 31 of 300 pages\n",
      "collecting for data+scientist from page 32 of 300 pages\n",
      "collecting for data+scientist from page 33 of 300 pages\n",
      "collecting for data+scientist from page 34 of 300 pages\n",
      "collecting for data+scientist from page 35 of 300 pages\n",
      "collecting for data+scientist from page 36 of 300 pages\n",
      "collecting for data+scientist from page 37 of 300 pages\n",
      "collecting for data+scientist from page 38 of 300 pages\n",
      "collecting for data+scientist from page 39 of 300 pages\n",
      "collecting for data+scientist from page 40 of 300 pages\n",
      "collecting for data+scientist from page 41 of 300 pages\n",
      "collecting for data+scientist from page 42 of 300 pages\n",
      "collecting for data+scientist from page 43 of 300 pages\n",
      "collecting for data+scientist from page 44 of 300 pages\n",
      "collecting for data+scientist from page 45 of 300 pages\n",
      "collecting for data+scientist from page 46 of 300 pages\n",
      "collecting for data+scientist from page 47 of 300 pages\n",
      "collecting for data+scientist from page 48 of 300 pages\n",
      "collecting for data+scientist from page 49 of 300 pages\n",
      "collecting for data+scientist from page 50 of 300 pages\n",
      "collecting for data+scientist from page 51 of 300 pages\n",
      "collecting for data+scientist from page 52 of 300 pages\n",
      "collecting for data+scientist from page 53 of 300 pages\n",
      "collecting for data+scientist from page 54 of 300 pages\n",
      "collecting for data+scientist from page 55 of 300 pages\n",
      "collecting for data+scientist from page 56 of 300 pages\n",
      "collecting for data+scientist from page 57 of 300 pages\n",
      "collecting for data+scientist from page 58 of 300 pages\n",
      "collecting for data+scientist from page 59 of 300 pages\n",
      "collecting for data+scientist from page 60 of 300 pages\n",
      "collecting for data+scientist from page 61 of 300 pages\n",
      "collecting for data+scientist from page 62 of 300 pages\n",
      "collecting for data+scientist from page 63 of 300 pages\n",
      "collecting for data+scientist from page 64 of 300 pages\n",
      "collecting for data+scientist from page 65 of 300 pages\n",
      "collecting for data+scientist from page 66 of 300 pages\n",
      "collecting for data+scientist from page 67 of 300 pages\n",
      "collecting for data+scientist from page 68 of 300 pages\n",
      "collecting for data+scientist from page 69 of 300 pages\n",
      "collecting for data+scientist from page 70 of 300 pages\n",
      "collecting for data+scientist from page 71 of 300 pages\n",
      "collecting for data+scientist from page 72 of 300 pages\n",
      "collecting for data+scientist from page 73 of 300 pages\n",
      "collecting for data+scientist from page 74 of 300 pages\n",
      "collecting for data+scientist from page 75 of 300 pages\n",
      "collecting for data+scientist from page 76 of 300 pages\n",
      "collecting for data+scientist from page 77 of 300 pages\n",
      "collecting for data+scientist from page 78 of 300 pages\n",
      "collecting for data+scientist from page 79 of 300 pages\n",
      "collecting for data+scientist from page 80 of 300 pages\n",
      "collecting for data+scientist from page 81 of 300 pages\n",
      "collecting for data+scientist from page 82 of 300 pages\n",
      "collecting for data+scientist from page 83 of 300 pages\n",
      "collecting for data+scientist from page 84 of 300 pages\n",
      "collecting for data+scientist from page 85 of 300 pages\n",
      "collecting for data+scientist from page 86 of 300 pages\n",
      "collecting for data+scientist from page 87 of 300 pages\n",
      "collecting for data+scientist from page 88 of 300 pages\n",
      "collecting for data+scientist from page 89 of 300 pages\n",
      "collecting for data+scientist from page 90 of 300 pages\n",
      "collecting for data+scientist from page 91 of 300 pages\n",
      "collecting for data+scientist from page 92 of 300 pages\n",
      "collecting for data+scientist from page 93 of 300 pages\n",
      "collecting for data+scientist from page 94 of 300 pages\n",
      "collecting for data+scientist from page 95 of 300 pages\n",
      "collecting for data+scientist from page 96 of 300 pages\n",
      "collecting for data+scientist from page 97 of 300 pages\n",
      "collecting for data+scientist from page 98 of 300 pages\n",
      "collecting for data+scientist from page 99 of 300 pages\n",
      "collecting for data+scientist from page 100 of 300 pages\n",
      "collecting for data+scientist from page 101 of 300 pages\n",
      "collecting for data+scientist from page 102 of 300 pages\n",
      "collecting for data+scientist from page 103 of 300 pages\n",
      "collecting for data+scientist from page 104 of 300 pages\n",
      "collecting for data+scientist from page 105 of 300 pages\n",
      "collecting for data+scientist from page 106 of 300 pages\n",
      "collecting for data+scientist from page 107 of 300 pages\n",
      "collecting for data+scientist from page 108 of 300 pages\n",
      "collecting for data+scientist from page 109 of 300 pages\n",
      "collecting for data+scientist from page 110 of 300 pages\n",
      "collecting for data+scientist from page 111 of 300 pages\n",
      "collecting for data+scientist from page 112 of 300 pages\n",
      "collecting for data+scientist from page 113 of 300 pages\n",
      "collecting for data+scientist from page 114 of 300 pages\n",
      "collecting for data+scientist from page 115 of 300 pages\n",
      "collecting for data+scientist from page 116 of 300 pages\n",
      "collecting for data+scientist from page 117 of 300 pages\n",
      "collecting for data+scientist from page 118 of 300 pages\n",
      "collecting for data+scientist from page 119 of 300 pages\n",
      "collecting for data+scientist from page 120 of 300 pages\n",
      "collecting for data+scientist from page 121 of 300 pages\n",
      "collecting for data+scientist from page 122 of 300 pages\n",
      "collecting for data+scientist from page 123 of 300 pages\n",
      "collecting for data+scientist from page 124 of 300 pages\n",
      "collecting for data+scientist from page 125 of 300 pages\n",
      "collecting for data+scientist from page 126 of 300 pages\n",
      "collecting for data+scientist from page 127 of 300 pages\n",
      "collecting for data+scientist from page 128 of 300 pages\n",
      "collecting for data+scientist from page 129 of 300 pages\n",
      "collecting for data+scientist from page 130 of 300 pages\n",
      "collecting for data+scientist from page 131 of 300 pages\n",
      "collecting for data+scientist from page 132 of 300 pages\n",
      "collecting for data+scientist from page 133 of 300 pages\n",
      "collecting for data+scientist from page 134 of 300 pages\n",
      "collecting for data+scientist from page 135 of 300 pages\n",
      "collecting for data+scientist from page 136 of 300 pages\n",
      "collecting for data+scientist from page 137 of 300 pages\n",
      "collecting for data+scientist from page 138 of 300 pages\n",
      "collecting for data+scientist from page 139 of 300 pages\n",
      "collecting for data+scientist from page 140 of 300 pages\n",
      "collecting for data+scientist from page 141 of 300 pages\n",
      "collecting for data+scientist from page 142 of 300 pages\n",
      "collecting for data+scientist from page 143 of 300 pages\n",
      "collecting for data+scientist from page 144 of 300 pages\n",
      "collecting for data+scientist from page 145 of 300 pages\n",
      "collecting for data+scientist from page 146 of 300 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for data+scientist from page 147 of 300 pages\n",
      "collecting for data+scientist from page 148 of 300 pages\n",
      "collecting for data+scientist from page 149 of 300 pages\n",
      "collecting for data+scientist from page 150 of 300 pages\n",
      "collecting for data+scientist from page 151 of 300 pages\n",
      "collecting for data+scientist from page 152 of 300 pages\n",
      "collecting for data+scientist from page 153 of 300 pages\n",
      "collecting for data+scientist from page 154 of 300 pages\n",
      "collecting for data+scientist from page 155 of 300 pages\n",
      "collecting for data+scientist from page 156 of 300 pages\n",
      "collecting for data+scientist from page 157 of 300 pages\n",
      "collecting for data+scientist from page 158 of 300 pages\n",
      "collecting for data+scientist from page 159 of 300 pages\n",
      "collecting for data+scientist from page 160 of 300 pages\n",
      "collecting for data+scientist from page 161 of 300 pages\n",
      "collecting for data+scientist from page 162 of 300 pages\n",
      "collecting for data+scientist from page 163 of 300 pages\n",
      "collecting for data+scientist from page 164 of 300 pages\n",
      "collecting for data+scientist from page 165 of 300 pages\n",
      "collecting for data+scientist from page 166 of 300 pages\n",
      "collecting for data+scientist from page 167 of 300 pages\n",
      "collecting for data+scientist from page 168 of 300 pages\n",
      "collecting for data+scientist from page 169 of 300 pages\n",
      "collecting for data+scientist from page 170 of 300 pages\n",
      "collecting for data+scientist from page 171 of 300 pages\n",
      "collecting for data+scientist from page 172 of 300 pages\n",
      "collecting for data+scientist from page 173 of 300 pages\n",
      "collecting for data+scientist from page 174 of 300 pages\n",
      "collecting for data+scientist from page 175 of 300 pages\n",
      "collecting for data+scientist from page 176 of 300 pages\n",
      "collecting for data+scientist from page 177 of 300 pages\n",
      "collecting for data+scientist from page 178 of 300 pages\n",
      "collecting for data+scientist from page 179 of 300 pages\n",
      "collecting for data+scientist from page 180 of 300 pages\n",
      "collecting for data+scientist from page 181 of 300 pages\n",
      "collecting for data+scientist from page 182 of 300 pages\n",
      "collecting for data+scientist from page 183 of 300 pages\n",
      "collecting for data+scientist from page 184 of 300 pages\n",
      "collecting for data+scientist from page 185 of 300 pages\n",
      "collecting for data+scientist from page 186 of 300 pages\n",
      "collecting for data+scientist from page 187 of 300 pages\n",
      "collecting for data+scientist from page 188 of 300 pages\n",
      "collecting for data+scientist from page 189 of 300 pages\n",
      "collecting for data+scientist from page 190 of 300 pages\n",
      "collecting for data+scientist from page 191 of 300 pages\n",
      "collecting for data+scientist from page 192 of 300 pages\n",
      "collecting for data+scientist from page 193 of 300 pages\n",
      "collecting for data+scientist from page 194 of 300 pages\n",
      "collecting for data+scientist from page 195 of 300 pages\n",
      "collecting for data+scientist from page 196 of 300 pages\n",
      "collecting for data+scientist from page 197 of 300 pages\n",
      "collecting for data+scientist from page 198 of 300 pages\n",
      "collecting for data+scientist from page 199 of 300 pages\n",
      "collecting for data+scientist from page 200 of 300 pages\n",
      "collecting for data+scientist from page 201 of 300 pages\n",
      "collecting for data+scientist from page 202 of 300 pages\n",
      "collecting for data+scientist from page 203 of 300 pages\n",
      "collecting for data+scientist from page 204 of 300 pages\n",
      "collecting for data+scientist from page 205 of 300 pages\n",
      "collecting for data+scientist from page 206 of 300 pages\n",
      "collecting for data+scientist from page 207 of 300 pages\n",
      "collecting for data+scientist from page 208 of 300 pages\n",
      "collecting for data+scientist from page 209 of 300 pages\n",
      "collecting for data+scientist from page 210 of 300 pages\n",
      "collecting for data+scientist from page 211 of 300 pages\n",
      "collecting for data+scientist from page 212 of 300 pages\n",
      "collecting for data+scientist from page 213 of 300 pages\n",
      "collecting for data+scientist from page 214 of 300 pages\n",
      "collecting for data+scientist from page 215 of 300 pages\n",
      "collecting for data+scientist from page 216 of 300 pages\n",
      "collecting for data+scientist from page 217 of 300 pages\n",
      "collecting for data+scientist from page 218 of 300 pages\n",
      "collecting for data+scientist from page 219 of 300 pages\n",
      "collecting for data+scientist from page 220 of 300 pages\n",
      "collecting for data+scientist from page 221 of 300 pages\n",
      "collecting for data+scientist from page 222 of 300 pages\n",
      "collecting for data+scientist from page 223 of 300 pages\n",
      "collecting for data+scientist from page 224 of 300 pages\n",
      "collecting for data+scientist from page 225 of 300 pages\n",
      "collecting for data+scientist from page 226 of 300 pages\n",
      "collecting for data+scientist from page 227 of 300 pages\n",
      "collecting for data+scientist from page 228 of 300 pages\n",
      "collecting for data+scientist from page 229 of 300 pages\n",
      "collecting for data+scientist from page 230 of 300 pages\n",
      "collecting for data+scientist from page 231 of 300 pages\n",
      "collecting for data+scientist from page 232 of 300 pages\n",
      "collecting for data+scientist from page 233 of 300 pages\n",
      "collecting for data+scientist from page 234 of 300 pages\n",
      "collecting for data+scientist from page 235 of 300 pages\n",
      "collecting for data+scientist from page 236 of 300 pages\n",
      "collecting for data+scientist from page 237 of 300 pages\n",
      "collecting for data+scientist from page 238 of 300 pages\n",
      "collecting for data+scientist from page 239 of 300 pages\n",
      "collecting for data+scientist from page 240 of 300 pages\n",
      "collecting for data+scientist from page 241 of 300 pages\n",
      "collecting for data+scientist from page 242 of 300 pages\n",
      "collecting for data+scientist from page 243 of 300 pages\n",
      "collecting for data+scientist from page 244 of 300 pages\n",
      "collecting for data+scientist from page 245 of 300 pages\n",
      "collecting for data+scientist from page 246 of 300 pages\n",
      "collecting for data+scientist from page 247 of 300 pages\n",
      "collecting for data+scientist from page 248 of 300 pages\n",
      "collecting for data+scientist from page 249 of 300 pages\n",
      "collecting for data+scientist from page 250 of 300 pages\n",
      "collecting for data+scientist from page 251 of 300 pages\n",
      "collecting for data+scientist from page 252 of 300 pages\n",
      "collecting for data+scientist from page 253 of 300 pages\n",
      "collecting for data+scientist from page 254 of 300 pages\n",
      "collecting for data+scientist from page 255 of 300 pages\n",
      "collecting for data+scientist from page 256 of 300 pages\n",
      "collecting for data+scientist from page 257 of 300 pages\n",
      "collecting for data+scientist from page 258 of 300 pages\n",
      "collecting for data+scientist from page 259 of 300 pages\n",
      "collecting for data+scientist from page 260 of 300 pages\n",
      "collecting for data+scientist from page 261 of 300 pages\n",
      "collecting for data+scientist from page 262 of 300 pages\n",
      "collecting for data+scientist from page 263 of 300 pages\n",
      "collecting for data+scientist from page 264 of 300 pages\n",
      "collecting for data+scientist from page 265 of 300 pages\n",
      "collecting for data+scientist from page 266 of 300 pages\n",
      "collecting for data+scientist from page 267 of 300 pages\n",
      "collecting for data+scientist from page 268 of 300 pages\n",
      "collecting for data+scientist from page 269 of 300 pages\n",
      "collecting for data+scientist from page 270 of 300 pages\n",
      "collecting for data+scientist from page 271 of 300 pages\n",
      "collecting for data+scientist from page 272 of 300 pages\n",
      "collecting for data+scientist from page 273 of 300 pages\n",
      "collecting for data+scientist from page 274 of 300 pages\n",
      "collecting for data+scientist from page 275 of 300 pages\n",
      "collecting for data+scientist from page 276 of 300 pages\n",
      "collecting for data+scientist from page 277 of 300 pages\n",
      "collecting for data+scientist from page 278 of 300 pages\n",
      "collecting for data+scientist from page 279 of 300 pages\n",
      "collecting for data+scientist from page 280 of 300 pages\n",
      "collecting for data+scientist from page 281 of 300 pages\n",
      "collecting for data+scientist from page 282 of 300 pages\n",
      "collecting for data+scientist from page 283 of 300 pages\n",
      "collecting for data+scientist from page 284 of 300 pages\n",
      "collecting for data+scientist from page 285 of 300 pages\n",
      "collecting for data+scientist from page 286 of 300 pages\n",
      "collecting for data+scientist from page 287 of 300 pages\n",
      "collecting for data+scientist from page 288 of 300 pages\n",
      "collecting for data+scientist from page 289 of 300 pages\n",
      "collecting for data+scientist from page 290 of 300 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for data+scientist from page 291 of 300 pages\n",
      "collecting for data+scientist from page 292 of 300 pages\n",
      "collecting for data+scientist from page 293 of 300 pages\n",
      "collecting for data+scientist from page 294 of 300 pages\n",
      "collecting for data+scientist from page 295 of 300 pages\n",
      "collecting for data+scientist from page 296 of 300 pages\n",
      "collecting for data+scientist from page 297 of 300 pages\n",
      "collecting for data+scientist from page 298 of 300 pages\n",
      "collecting for data+scientist from page 299 of 300 pages\n",
      "collecting for data+scientist from page 300 of 300 pages\n",
      "collecting for data+analyst from page 1 of 300 pages\n",
      "collecting for data+analyst from page 2 of 300 pages\n",
      "collecting for data+analyst from page 3 of 300 pages\n",
      "collecting for data+analyst from page 4 of 300 pages\n",
      "collecting for data+analyst from page 5 of 300 pages\n",
      "collecting for data+analyst from page 6 of 300 pages\n",
      "collecting for data+analyst from page 7 of 300 pages\n",
      "collecting for data+analyst from page 8 of 300 pages\n",
      "collecting for data+analyst from page 9 of 300 pages\n",
      "collecting for data+analyst from page 10 of 300 pages\n",
      "collecting for data+analyst from page 11 of 300 pages\n",
      "collecting for data+analyst from page 12 of 300 pages\n",
      "collecting for data+analyst from page 13 of 300 pages\n",
      "collecting for data+analyst from page 14 of 300 pages\n",
      "collecting for data+analyst from page 15 of 300 pages\n",
      "collecting for data+analyst from page 16 of 300 pages\n",
      "collecting for data+analyst from page 17 of 300 pages\n",
      "collecting for data+analyst from page 18 of 300 pages\n",
      "collecting for data+analyst from page 19 of 300 pages\n",
      "collecting for data+analyst from page 20 of 300 pages\n",
      "collecting for data+analyst from page 21 of 300 pages\n",
      "collecting for data+analyst from page 22 of 300 pages\n",
      "collecting for data+analyst from page 23 of 300 pages\n",
      "collecting for data+analyst from page 24 of 300 pages\n",
      "collecting for data+analyst from page 25 of 300 pages\n",
      "collecting for data+analyst from page 26 of 300 pages\n",
      "collecting for data+analyst from page 27 of 300 pages\n",
      "collecting for data+analyst from page 28 of 300 pages\n",
      "collecting for data+analyst from page 29 of 300 pages\n",
      "collecting for data+analyst from page 30 of 300 pages\n",
      "collecting for data+analyst from page 31 of 300 pages\n",
      "collecting for data+analyst from page 32 of 300 pages\n",
      "collecting for data+analyst from page 33 of 300 pages\n",
      "collecting for data+analyst from page 34 of 300 pages\n",
      "collecting for data+analyst from page 35 of 300 pages\n",
      "collecting for data+analyst from page 36 of 300 pages\n",
      "collecting for data+analyst from page 37 of 300 pages\n",
      "collecting for data+analyst from page 38 of 300 pages\n",
      "collecting for data+analyst from page 39 of 300 pages\n",
      "collecting for data+analyst from page 40 of 300 pages\n",
      "collecting for data+analyst from page 41 of 300 pages\n",
      "collecting for data+analyst from page 42 of 300 pages\n",
      "collecting for data+analyst from page 43 of 300 pages\n",
      "collecting for data+analyst from page 44 of 300 pages\n",
      "collecting for data+analyst from page 45 of 300 pages\n",
      "collecting for data+analyst from page 46 of 300 pages\n",
      "collecting for data+analyst from page 47 of 300 pages\n",
      "collecting for data+analyst from page 48 of 300 pages\n",
      "collecting for data+analyst from page 49 of 300 pages\n",
      "collecting for data+analyst from page 50 of 300 pages\n",
      "collecting for data+analyst from page 51 of 300 pages\n",
      "collecting for data+analyst from page 52 of 300 pages\n",
      "collecting for data+analyst from page 53 of 300 pages\n",
      "collecting for data+analyst from page 54 of 300 pages\n",
      "collecting for data+analyst from page 55 of 300 pages\n",
      "collecting for data+analyst from page 56 of 300 pages\n",
      "collecting for data+analyst from page 57 of 300 pages\n",
      "collecting for data+analyst from page 58 of 300 pages\n",
      "collecting for data+analyst from page 59 of 300 pages\n",
      "collecting for data+analyst from page 60 of 300 pages\n",
      "collecting for data+analyst from page 61 of 300 pages\n",
      "collecting for data+analyst from page 62 of 300 pages\n",
      "collecting for data+analyst from page 63 of 300 pages\n",
      "collecting for data+analyst from page 64 of 300 pages\n",
      "collecting for data+analyst from page 65 of 300 pages\n",
      "collecting for data+analyst from page 66 of 300 pages\n",
      "collecting for data+analyst from page 67 of 300 pages\n",
      "collecting for data+analyst from page 68 of 300 pages\n",
      "collecting for data+analyst from page 69 of 300 pages\n",
      "collecting for data+analyst from page 70 of 300 pages\n",
      "collecting for data+analyst from page 71 of 300 pages\n",
      "collecting for data+analyst from page 72 of 300 pages\n",
      "collecting for data+analyst from page 73 of 300 pages\n",
      "collecting for data+analyst from page 74 of 300 pages\n",
      "collecting for data+analyst from page 75 of 300 pages\n",
      "collecting for data+analyst from page 76 of 300 pages\n",
      "collecting for data+analyst from page 77 of 300 pages\n",
      "collecting for data+analyst from page 78 of 300 pages\n",
      "collecting for data+analyst from page 79 of 300 pages\n",
      "collecting for data+analyst from page 80 of 300 pages\n",
      "collecting for data+analyst from page 81 of 300 pages\n",
      "collecting for data+analyst from page 82 of 300 pages\n",
      "collecting for data+analyst from page 83 of 300 pages\n",
      "collecting for data+analyst from page 84 of 300 pages\n",
      "collecting for data+analyst from page 85 of 300 pages\n",
      "collecting for data+analyst from page 86 of 300 pages\n",
      "collecting for data+analyst from page 87 of 300 pages\n",
      "collecting for data+analyst from page 88 of 300 pages\n",
      "collecting for data+analyst from page 89 of 300 pages\n",
      "collecting for data+analyst from page 90 of 300 pages\n",
      "collecting for data+analyst from page 91 of 300 pages\n",
      "collecting for data+analyst from page 92 of 300 pages\n",
      "collecting for data+analyst from page 93 of 300 pages\n",
      "collecting for data+analyst from page 94 of 300 pages\n",
      "collecting for data+analyst from page 95 of 300 pages\n",
      "collecting for data+analyst from page 96 of 300 pages\n",
      "collecting for data+analyst from page 97 of 300 pages\n",
      "collecting for data+analyst from page 98 of 300 pages\n",
      "collecting for data+analyst from page 99 of 300 pages\n",
      "collecting for data+analyst from page 100 of 300 pages\n",
      "collecting for data+analyst from page 101 of 300 pages\n",
      "collecting for data+analyst from page 102 of 300 pages\n",
      "collecting for data+analyst from page 103 of 300 pages\n",
      "collecting for data+analyst from page 104 of 300 pages\n",
      "collecting for data+analyst from page 105 of 300 pages\n",
      "collecting for data+analyst from page 106 of 300 pages\n",
      "collecting for data+analyst from page 107 of 300 pages\n",
      "collecting for data+analyst from page 108 of 300 pages\n",
      "collecting for data+analyst from page 109 of 300 pages\n",
      "collecting for data+analyst from page 110 of 300 pages\n",
      "collecting for data+analyst from page 111 of 300 pages\n",
      "collecting for data+analyst from page 112 of 300 pages\n",
      "collecting for data+analyst from page 113 of 300 pages\n",
      "collecting for data+analyst from page 114 of 300 pages\n",
      "collecting for data+analyst from page 115 of 300 pages\n",
      "collecting for data+analyst from page 116 of 300 pages\n",
      "collecting for data+analyst from page 117 of 300 pages\n",
      "collecting for data+analyst from page 118 of 300 pages\n",
      "collecting for data+analyst from page 119 of 300 pages\n",
      "collecting for data+analyst from page 120 of 300 pages\n",
      "collecting for data+analyst from page 121 of 300 pages\n",
      "collecting for data+analyst from page 122 of 300 pages\n",
      "collecting for data+analyst from page 123 of 300 pages\n",
      "collecting for data+analyst from page 124 of 300 pages\n",
      "collecting for data+analyst from page 125 of 300 pages\n",
      "collecting for data+analyst from page 126 of 300 pages\n",
      "collecting for data+analyst from page 127 of 300 pages\n",
      "collecting for data+analyst from page 128 of 300 pages\n",
      "collecting for data+analyst from page 129 of 300 pages\n",
      "collecting for data+analyst from page 130 of 300 pages\n",
      "collecting for data+analyst from page 131 of 300 pages\n",
      "collecting for data+analyst from page 132 of 300 pages\n",
      "collecting for data+analyst from page 133 of 300 pages\n",
      "collecting for data+analyst from page 134 of 300 pages\n",
      "collecting for data+analyst from page 135 of 300 pages\n",
      "collecting for data+analyst from page 136 of 300 pages\n",
      "collecting for data+analyst from page 137 of 300 pages\n",
      "collecting for data+analyst from page 138 of 300 pages\n",
      "collecting for data+analyst from page 139 of 300 pages\n",
      "collecting for data+analyst from page 140 of 300 pages\n",
      "collecting for data+analyst from page 141 of 300 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for data+analyst from page 142 of 300 pages\n",
      "collecting for data+analyst from page 143 of 300 pages\n",
      "collecting for data+analyst from page 144 of 300 pages\n",
      "collecting for data+analyst from page 145 of 300 pages\n",
      "collecting for data+analyst from page 146 of 300 pages\n",
      "collecting for data+analyst from page 147 of 300 pages\n",
      "collecting for data+analyst from page 148 of 300 pages\n",
      "collecting for data+analyst from page 149 of 300 pages\n",
      "collecting for data+analyst from page 150 of 300 pages\n",
      "collecting for data+analyst from page 151 of 300 pages\n",
      "collecting for data+analyst from page 152 of 300 pages\n",
      "collecting for data+analyst from page 153 of 300 pages\n",
      "collecting for data+analyst from page 154 of 300 pages\n",
      "collecting for data+analyst from page 155 of 300 pages\n",
      "collecting for data+analyst from page 156 of 300 pages\n",
      "collecting for data+analyst from page 157 of 300 pages\n",
      "collecting for data+analyst from page 158 of 300 pages\n",
      "collecting for data+analyst from page 159 of 300 pages\n",
      "collecting for data+analyst from page 160 of 300 pages\n",
      "collecting for data+analyst from page 161 of 300 pages\n",
      "collecting for data+analyst from page 162 of 300 pages\n",
      "collecting for data+analyst from page 163 of 300 pages\n",
      "collecting for data+analyst from page 164 of 300 pages\n",
      "collecting for data+analyst from page 165 of 300 pages\n",
      "collecting for data+analyst from page 166 of 300 pages\n",
      "collecting for data+analyst from page 167 of 300 pages\n",
      "collecting for data+analyst from page 168 of 300 pages\n",
      "collecting for data+analyst from page 169 of 300 pages\n",
      "collecting for data+analyst from page 170 of 300 pages\n",
      "collecting for data+analyst from page 171 of 300 pages\n",
      "collecting for data+analyst from page 172 of 300 pages\n",
      "collecting for data+analyst from page 173 of 300 pages\n",
      "collecting for data+analyst from page 174 of 300 pages\n",
      "collecting for data+analyst from page 175 of 300 pages\n",
      "collecting for data+analyst from page 176 of 300 pages\n",
      "collecting for data+analyst from page 177 of 300 pages\n",
      "collecting for data+analyst from page 178 of 300 pages\n",
      "collecting for data+analyst from page 179 of 300 pages\n",
      "collecting for data+analyst from page 180 of 300 pages\n",
      "collecting for data+analyst from page 181 of 300 pages\n",
      "collecting for data+analyst from page 182 of 300 pages\n",
      "collecting for data+analyst from page 183 of 300 pages\n",
      "collecting for data+analyst from page 184 of 300 pages\n",
      "collecting for data+analyst from page 185 of 300 pages\n",
      "collecting for data+analyst from page 186 of 300 pages\n",
      "collecting for data+analyst from page 187 of 300 pages\n",
      "collecting for data+analyst from page 188 of 300 pages\n",
      "collecting for data+analyst from page 189 of 300 pages\n",
      "collecting for data+analyst from page 190 of 300 pages\n",
      "collecting for data+analyst from page 191 of 300 pages\n",
      "collecting for data+analyst from page 192 of 300 pages\n",
      "collecting for data+analyst from page 193 of 300 pages\n",
      "collecting for data+analyst from page 194 of 300 pages\n",
      "collecting for data+analyst from page 195 of 300 pages\n",
      "collecting for data+analyst from page 196 of 300 pages\n",
      "collecting for data+analyst from page 197 of 300 pages\n",
      "collecting for data+analyst from page 198 of 300 pages\n",
      "collecting for data+analyst from page 199 of 300 pages\n",
      "collecting for data+analyst from page 200 of 300 pages\n",
      "collecting for data+analyst from page 201 of 300 pages\n",
      "collecting for data+analyst from page 202 of 300 pages\n",
      "collecting for data+analyst from page 203 of 300 pages\n",
      "collecting for data+analyst from page 204 of 300 pages\n",
      "collecting for data+analyst from page 205 of 300 pages\n",
      "collecting for data+analyst from page 206 of 300 pages\n",
      "collecting for data+analyst from page 207 of 300 pages\n",
      "collecting for data+analyst from page 208 of 300 pages\n",
      "collecting for data+analyst from page 209 of 300 pages\n",
      "collecting for data+analyst from page 210 of 300 pages\n",
      "collecting for data+analyst from page 211 of 300 pages\n",
      "collecting for data+analyst from page 212 of 300 pages\n",
      "collecting for data+analyst from page 213 of 300 pages\n",
      "collecting for data+analyst from page 214 of 300 pages\n",
      "collecting for data+analyst from page 215 of 300 pages\n",
      "collecting for data+analyst from page 216 of 300 pages\n",
      "collecting for data+analyst from page 217 of 300 pages\n",
      "collecting for data+analyst from page 218 of 300 pages\n",
      "collecting for data+analyst from page 219 of 300 pages\n",
      "collecting for data+analyst from page 220 of 300 pages\n",
      "collecting for data+analyst from page 221 of 300 pages\n",
      "collecting for data+analyst from page 222 of 300 pages\n",
      "collecting for data+analyst from page 223 of 300 pages\n",
      "collecting for data+analyst from page 224 of 300 pages\n",
      "collecting for data+analyst from page 225 of 300 pages\n",
      "collecting for data+analyst from page 226 of 300 pages\n",
      "collecting for data+analyst from page 227 of 300 pages\n",
      "collecting for data+analyst from page 228 of 300 pages\n",
      "collecting for data+analyst from page 229 of 300 pages\n",
      "collecting for data+analyst from page 230 of 300 pages\n",
      "collecting for data+analyst from page 231 of 300 pages\n",
      "collecting for data+analyst from page 232 of 300 pages\n",
      "collecting for data+analyst from page 233 of 300 pages\n",
      "collecting for data+analyst from page 234 of 300 pages\n",
      "collecting for data+analyst from page 235 of 300 pages\n",
      "collecting for data+analyst from page 236 of 300 pages\n",
      "collecting for data+analyst from page 237 of 300 pages\n",
      "collecting for data+analyst from page 238 of 300 pages\n",
      "collecting for data+analyst from page 239 of 300 pages\n",
      "collecting for data+analyst from page 240 of 300 pages\n",
      "collecting for data+analyst from page 241 of 300 pages\n",
      "collecting for data+analyst from page 242 of 300 pages\n",
      "collecting for data+analyst from page 243 of 300 pages\n",
      "collecting for data+analyst from page 244 of 300 pages\n",
      "collecting for data+analyst from page 245 of 300 pages\n",
      "collecting for data+analyst from page 246 of 300 pages\n",
      "collecting for data+analyst from page 247 of 300 pages\n",
      "collecting for data+analyst from page 248 of 300 pages\n",
      "collecting for data+analyst from page 249 of 300 pages\n",
      "collecting for data+analyst from page 250 of 300 pages\n",
      "collecting for data+analyst from page 251 of 300 pages\n",
      "collecting for data+analyst from page 252 of 300 pages\n",
      "collecting for data+analyst from page 253 of 300 pages\n",
      "collecting for data+analyst from page 254 of 300 pages\n",
      "collecting for data+analyst from page 255 of 300 pages\n",
      "collecting for data+analyst from page 256 of 300 pages\n",
      "collecting for data+analyst from page 257 of 300 pages\n",
      "collecting for data+analyst from page 258 of 300 pages\n",
      "collecting for data+analyst from page 259 of 300 pages\n",
      "collecting for data+analyst from page 260 of 300 pages\n",
      "collecting for data+analyst from page 261 of 300 pages\n",
      "collecting for data+analyst from page 262 of 300 pages\n",
      "collecting for data+analyst from page 263 of 300 pages\n",
      "collecting for data+analyst from page 264 of 300 pages\n",
      "collecting for data+analyst from page 265 of 300 pages\n",
      "collecting for data+analyst from page 266 of 300 pages\n",
      "collecting for data+analyst from page 267 of 300 pages\n",
      "collecting for data+analyst from page 268 of 300 pages\n",
      "collecting for data+analyst from page 269 of 300 pages\n",
      "collecting for data+analyst from page 270 of 300 pages\n",
      "collecting for data+analyst from page 271 of 300 pages\n",
      "collecting for data+analyst from page 272 of 300 pages\n",
      "collecting for data+analyst from page 273 of 300 pages\n",
      "collecting for data+analyst from page 274 of 300 pages\n",
      "collecting for data+analyst from page 275 of 300 pages\n",
      "collecting for data+analyst from page 276 of 300 pages\n",
      "collecting for data+analyst from page 277 of 300 pages\n",
      "collecting for data+analyst from page 278 of 300 pages\n",
      "collecting for data+analyst from page 279 of 300 pages\n",
      "collecting for data+analyst from page 280 of 300 pages\n",
      "collecting for data+analyst from page 281 of 300 pages\n",
      "collecting for data+analyst from page 282 of 300 pages\n",
      "collecting for data+analyst from page 283 of 300 pages\n",
      "collecting for data+analyst from page 284 of 300 pages\n",
      "collecting for data+analyst from page 285 of 300 pages\n",
      "collecting for data+analyst from page 286 of 300 pages\n",
      "collecting for data+analyst from page 287 of 300 pages\n",
      "collecting for data+analyst from page 288 of 300 pages\n",
      "collecting for data+analyst from page 289 of 300 pages\n",
      "collecting for data+analyst from page 290 of 300 pages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting for data+analyst from page 291 of 300 pages\n",
      "collecting for data+analyst from page 292 of 300 pages\n",
      "collecting for data+analyst from page 293 of 300 pages\n",
      "collecting for data+analyst from page 294 of 300 pages\n",
      "collecting for data+analyst from page 295 of 300 pages\n",
      "collecting for data+analyst from page 296 of 300 pages\n",
      "collecting for data+analyst from page 297 of 300 pages\n",
      "collecting for data+analyst from page 298 of 300 pages\n",
      "collecting for data+analyst from page 299 of 300 pages\n",
      "collecting for data+analyst from page 300 of 300 pages\n",
      "Number of unique urls: 5221, number of all urls collected: 10078\n"
     ]
    }
   ],
   "source": [
    "# Collect data for San Jose:\n",
    "\n",
    "# Scraping parameters:\n",
    "web_site = 'https://www.indeed.com/'\n",
    "city = 'San+Jose'\n",
    "job_title_list = ['data+scientist',\n",
    "                  'data+analyst']\n",
    "state = 'ca'\n",
    "\n",
    "san_jose_urls = scrape_urls(web_site, city, job_title_list, state)\n",
    "\n",
    "# Pickle url list:\n",
    "pickling_in = open('/Users/greenapple/project4/data/urls/san_jose_urls_111019.pkl', 'wb')\n",
    "pickle.dump(san_jose_urls, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-11T07:39:56.470205Z",
     "start_time": "2019-11-11T07:39:56.454027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5221"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(san_jose_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for South San Francisco:\n",
    "\n",
    "# Scraping parameters:\n",
    "web_site = 'https://www.indeed.com/'\n",
    "city = 'South+San+Francisco'\n",
    "job_title_list = ['data+scientist',\n",
    "                  'data+analyst']\n",
    "state = 'ca'\n",
    "\n",
    "south_sf_urls = scrape_urls(web_site, city, job_title_list, state)\n",
    "\n",
    "# Pickle url list:\n",
    "pickling_in = open('south_sf_urls_110519.pkl', 'wb')\n",
    "pickle.dump(south_sf_urls, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(south_sf_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data for Palo Alto:\n",
    "\n",
    "# Scraping parameters:\n",
    "web_site = 'https://www.indeed.com/'\n",
    "city = 'Palo+Alto'\n",
    "job_title_list = ['data+scientist',\n",
    "                  'data+analyst']\n",
    "state = 'ca'\n",
    "\n",
    "palo_alto_urls = scrape_urls(web_site, city, job_title_list, state)\n",
    "\n",
    "# Pickle url list:\n",
    "pickling_in = open('palo_alto_urls_110519.pkl', 'wb')\n",
    "pickle.dump(palo_alto_urls, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(palo_alto_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CA urls and make sure they are unique:\n",
    "california_job_urls = san_jose_urls.union(san_jose_urls, palo_alto_urls)\n",
    "len(california_job_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project4] *",
   "language": "python",
   "name": "conda-env-project4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

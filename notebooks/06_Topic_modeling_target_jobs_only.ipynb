{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-06T23:22:53.817308Z",
     "start_time": "2019-11-06T23:21:37.158885Z"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en\n",
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T14:41:40.679915Z",
     "start_time": "2019-11-09T14:41:37.357400Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import nltk\n",
    "import textcleaner\n",
    "import pickle\n",
    "import spacy\n",
    "import jieba\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textblob import TextBlob\n",
    "import textcleaner as tc\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T15:07:28.642439Z",
     "start_time": "2019-11-09T15:07:28.420335Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greenapple/anaconda3/envs/project4e/lib/python3.7/site-packages/ipykernel_launcher.py:23: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load scraped job description data\n",
    "\n",
    "file_list = [\n",
    "'data_110619_seattle_0to1k.pkl',\n",
    "'data_110619_seattle_1kto2k.pkl',\n",
    "'data_110619_seattle_2kto3k.pkl',\n",
    "'data_110619_seattle_neg_5.pkl',\n",
    "'data_110619_seattle_3kto4k.pkl',\n",
    "'data_110619_seattle_4ktoend.pkl',\n",
    "'data_110619_bellevue_0to1k.pkl',\n",
    "'data_110619_bellevue_1to2k.pkl',\n",
    "'data_110619_bellevue_2to3k.pkl',\n",
    "'data_110619_bellevue_3toend.pkl'            ]\n",
    "jobs_target = pd.DataFrame()\n",
    "\n",
    "for file in file_list:\n",
    "    \n",
    "    folder = '/Users/greenapple/project4/data/interim/'\n",
    "    path = os.path.join(folder, file)\n",
    "    \n",
    "    pickling_out = open(path, 'rb')\n",
    "    df_piece = pickle.load(pickling_out)\n",
    "    jobs_target = pd.concat([jobs_target, df_piece]) \n",
    "    \n",
    "    if file == 'data_110619_seattle_neg.pkl':\n",
    "        jobs_target['type'] = 'negative'\n",
    "    else:\n",
    "        jobs_target['type'] = 'positive'\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T15:07:32.984234Z",
     "start_time": "2019-11-09T15:07:32.920090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8699, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T15:07:47.897547Z",
     "start_time": "2019-11-09T15:07:47.745353Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8675, 7) (5058, 7)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicates and data rows that were inserted by indeed\n",
    "jobs_target.drop_duplicates(inplace=True)\n",
    "a = jobs_target.shape\n",
    "jobs_target.drop(jobs_target.loc[jobs_target.company_name == 'Seen by Indeed'].index, inplace=True)\n",
    "jobs_target.reset_index(drop=True, inplace=True)\n",
    "b = jobs_target.shape\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T18:22:32.494670Z",
     "start_time": "2019-11-08T18:22:32.490785Z"
    }
   },
   "outputs": [],
   "source": [
    "# Collect data for the rows replaced by indeed\n",
    "replaced_data_0to1k = data_110619_seattle_set1.loc[data_110619_seattle_set1.company_name=='Seen by Indeed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T21:58:48.018810Z",
     "start_time": "2019-11-08T21:58:48.007466Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load metis resumes\n",
    "pickling_out = open('/Users/greenapple/project4/data/resumes/resumes_110719', 'rb')\n",
    "resumes_l = pickle.load(pickling_out)\n",
    "\n",
    "resumes = pd.DataFrame(resumes_l, columns = ['description'])\n",
    "resumes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T00:12:59.978068Z",
     "start_time": "2019-11-08T00:12:59.973786Z"
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T22:51:41.984355Z",
     "start_time": "2019-11-08T22:51:41.977172Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-process text\n",
    "def pre_process(text):\n",
    "    '''\n",
    "    Takes a string, returns a cleaned string.\n",
    "    '''\n",
    "    text_sub = re.sub('[/.\\n:-]', ' ', text)  # Remove seaparators of dif kind\n",
    "    text_sub = re.sub('[,\\(\\)]', '', text_sub).lower()  # Remove \",\" and capital letters\n",
    "    text_sub = re.sub('[^a-z\\s]', '', text_sub) # Keep alphanumeric characters only\n",
    "#     re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "\n",
    "    return text_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T22:09:29.090381Z",
     "start_time": "2019-11-08T22:09:28.573943Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T22:50:35.354479Z",
     "start_time": "2019-11-08T22:50:35.346247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pre-process text\n",
    "from gensim.parsing.preprocessing import preprocess_string, strip_numeric, strip_short, stem_text, strip_tags, strip_punctuation, strip_multiple_whitespaces, remove_stopwords\n",
    "\n",
    "def gensim_preprocess(text):\n",
    "    '''\n",
    "    Takes a string, returns a list of cleaned strings.\n",
    "    '''\n",
    "    filters = [\n",
    "        strip_tags,\n",
    "        strip_punctuation, \n",
    "        strip_multiple_whitespaces, \n",
    "        strip_numeric, \n",
    "        remove_stopwords, \n",
    "        strip_short,\n",
    "        lambda x: x.lower()]\n",
    "    \n",
    "    text_proc = preprocess_string(text, filters)\n",
    "    return text_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T18:23:05.641772Z",
     "start_time": "2019-11-08T18:23:05.635605Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "def tokenize_lemma(text):\n",
    "        \n",
    "    # processer = spacy.load('en')\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "\n",
    "    text_obj = tokenizer(text)\n",
    "    text_str = ' '.join([token.lemma_ for token in text_obj if not token.is_stop])\n",
    "    return text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:18:01.514013Z",
     "start_time": "2019-11-09T03:18:00.805087Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text: nouns\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_noun(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_noun = ' '.join([token.text for token in text_obj if token.pos_=='NOUN'])\n",
    "  \n",
    "    return text_noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:22:37.742137Z",
     "start_time": "2019-11-09T03:22:37.170319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text: lemma and nouns\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_noun_lemma(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_noun_lemma = ' '.join([token.lemma_ for token in text_obj if token.pos_=='NOUN'])\n",
    "   \n",
    "  \n",
    "    return text_noun_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:31:13.698484Z",
     "start_time": "2019-11-09T03:31:13.065123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text: lemma\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_lemma(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_lemma = ' '.join([token.lemma_ for token in text_obj if not token.is_stop])\n",
    "  \n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:31:15.075949Z",
     "start_time": "2019-11-09T03:31:14.528850Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize text: words\n",
    "\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_word(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_text = ' '.join([token.text for token in text_obj if not token.is_stop])\n",
    "  \n",
    "    return text_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOES NOT WORK\n",
    "# Tokenize text: more choices\n",
    "\n",
    "# processer = spacy.load('en')\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "    \n",
    "def tokenize_multi(text):\n",
    "        \n",
    "    text_obj = tokenizer(text, disable=['parser', 'ner'])\n",
    "    \n",
    "    text_noun = ' '.join([token.text for token in text_obj if token.pos_=='NOUN'])\n",
    "    text_noun_lemma = ' '.join([token.lemma_ for token in text_obj if token.pos_=='NOUN'])\n",
    "    text_lemma = ' '.join([token.lemma_ for token in text_obj if not token.is_stop])\n",
    "    text_text = ' '.join([token.text for token in text_obj if not token.is_stop])\n",
    "  \n",
    "    return text_noun, text_noun_lemma, text_lemma, text_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:43:00.291867Z",
     "start_time": "2019-11-09T03:43:00.284487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize: tf-idf\n",
    "def tf_idf(text1, text2):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2),               # vectorizer\n",
    "                        stop_words='english', \n",
    "                        token_pattern=\"\\\\b[a-z][a-z]+\\\\b\")\n",
    "    X_tfidf = vectorizer.fit_transform(text1).toarray()            # vectorize text1\n",
    "    Z_tfidf = vectorizer.transform(text2).toarray()                # vectorize text1\n",
    "    \n",
    "    \n",
    "#     print('text1{}'.format(pd.DataFrame(X_tfidf, columns=tfidf.get_feature_names()).head(5)))\n",
    "#     print('text2{}'.format(pd.DataFrame(Z_tfidf, columns=tfidf.get_feature_names()).head(5)))\n",
    "    \n",
    "    return X_tfidf, Z_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:43:08.169018Z",
     "start_time": "2019-11-09T03:43:08.162247Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize: count\n",
    "def count_(text1, text2):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X_vect = vectorizer.fit_transform(text1).toarray()            # vectorize text1\n",
    "    Z_vect = vectorizer.transform(text2).toarray()                # vectorize text1\n",
    "    \n",
    "    \n",
    "#     print('text1{}'.format(pd.DataFrame(X_tfidf, columns=tfidf.get_feature_names()).head(5)))\n",
    "#     print('text2{}'.format(pd.DataFrame(Z_tfidf, columns=tfidf.get_feature_names()).head(5)))\n",
    "    \n",
    "    return X_vect, Z_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:47:53.234844Z",
     "start_time": "2019-11-09T03:47:53.228296Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce dimensionality:\n",
    "def lsa(X, Z, components):\n",
    "    lsa = TruncatedSVD(components)\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "    Z_lsa = lsa.transform(Z)\n",
    "  \n",
    "    return X_lsa, Z_lsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T21:14:15.733802Z",
     "start_time": "2019-11-08T21:14:15.727113Z"
    }
   },
   "outputs": [],
   "source": [
    "def similarity(X_red, Z_red):\n",
    "    score = cosine_similarity(X_red, Z_red).round(2)\n",
    "    \n",
    "    column_labels = ['Resume'+str(i) for i in range(1, R_lsa.shape[0]+1)]\n",
    "    row_labels = ['Job'+str(i) for i in range(1, Z_lsa.shape[0]+1)]\n",
    "   \n",
    "    similarity_df = pd.DataFrame(score, columns=column_labels, index=row_labels)\n",
    "#     print(similarity_df)\n",
    "        \n",
    "    return similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean text, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T00:44:38.727687Z",
     "start_time": "2019-11-08T23:49:03.972014Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-adcc2596bdfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Tokenize:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_noun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_multi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_noun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_multi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text_lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize_multi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4040\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4041\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4042\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-121-d0ba86f0f354>\u001b[0m in \u001b[0;36mtokenize_multi\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# processer = spacy.load('en')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtext_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;34m\"\"\"Load a model from an installed package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0mlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lang_factory\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lang\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mfactories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"factories\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab, make_doc, max_length, meta, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmake_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mfactory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mmake_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mcreate_tokenizer\u001b[0;34m(cls, nlp)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0msuffix_search\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix_search\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0minfix_finditer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfix_finditer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mtoken_match\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_match\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         )\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.make_fused_token\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get_by_orth\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/project4e/lib/python3.7/site-packages/spacy/lang/en/lex_attrs.py\u001b[0m in \u001b[0;36mlike_num\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_num_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Clean and tokenize\n",
    "jobs['clean_text'] = jobs['description'].apply(pre_process) # clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T00:45:36.218704Z",
     "start_time": "2019-11-09T00:45:34.859245Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clean\n",
    "jobs['clean_text'] = jobs['description'].apply(pre_process) # clean\n",
    "\n",
    "# Save cleaned text\n",
    "pickling_in = open('/Users/greenapple/project4/data/processed/jobs_clean.pkl', 'wb')\n",
    "pickle.dump(jobs, pickling_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:39:26.583982Z",
     "start_time": "2019-11-09T03:36:57.692068Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize:\n",
    "jobs['noun'] = jobs['clean_text'].apply(tokenize_noun)\n",
    "jobs['noun_lemma'] = jobs['clean_text'].apply(tokenize_noun_lemma)\n",
    "jobs['lemma'] = jobs['clean_text'].apply(tokenize_lemma)\n",
    "jobs['word'] = jobs['clean_text'].apply(tokenize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:39:37.502898Z",
     "start_time": "2019-11-09T03:39:37.493782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company_name', 'description', 'job_title', 'link', 'location',\n",
       "       'salary', 'type', 'clean_text', 'noun', 'noun_lemma', 'lemma', 'word'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:39:46.182869Z",
     "start_time": "2019-11-09T03:39:45.901298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save tokenized text\n",
    "pickling_in = open('/Users/greenapple/project4/data/processed/jobs_tokenized.pkl', 'wb')\n",
    "pickle.dump(jobs, pickling_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T03:44:22.215026Z",
     "start_time": "2019-11-09T03:44:08.184170Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorize: Count and TF-IDF\n",
    "job_tf_idf_noun, _ = tf_idf(jobs['noun'], jobs['noun'])  # TF-IDF Vectorizer\n",
    "job_count_noun, _ = count_(jobs['noun'], jobs['noun'])  # Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce dimentionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:19:40.397508Z",
     "start_time": "2019-11-09T06:19:37.493782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1223"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSA: noun\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                            max_df = 0.9,\n",
    "                            min_df = 0.05,\n",
    "                            stop_words = ['yearexperience', 'year', 'years', 'porch', 'work', 'home', 'term'])\n",
    "\n",
    "noun_vc = vectorizer.fit_transform(jobs.noun)\n",
    "\n",
    "len(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:20:34.937772Z",
     "start_time": "2019-11-09T06:20:34.525693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02064644, 0.09040681, 0.06895247, 0.05957077, 0.05476894,\n",
       "       0.05341638, 0.0498317 , 0.04515979, 0.04129339, 0.04040844,\n",
       "       0.0344565 , 0.03288877, 0.03081242, 0.02632454, 0.02560626,\n",
       "       0.02471311, 0.02374627, 0.01976032, 0.01691725, 0.01350298])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = 20\n",
    "lsa = TruncatedSVD(components)\n",
    "\n",
    "\n",
    "noun_topics = lsa.fit_transform(noun_vc.toarray())  # reduce dimensionality\n",
    "\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:25:36.391880Z",
     "start_time": "2019-11-09T06:25:36.387648Z"
    }
   },
   "outputs": [],
   "source": [
    "# pd.DataFrame(noun_vc.toarray(), columns=vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:20:39.237518Z",
     "start_time": "2019-11-09T06:20:39.222772Z"
    }
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words, topic_names=None):\n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix)\n",
    "        else:\n",
    "            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:20:40.279501Z",
     "start_time": "2019-11-09T06:20:40.261316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "data, team, business, science, data science, engineering, design, solutions, models, problems, analysis, ability, machine, customer, projects, services, management, time, project, skills\n",
      "\n",
      "Topic  1\n",
      "engineering, design, water, project, site, client, construction, drawings, engineer, transportation, traffic, firm, clients, specifications, development, management, position, employees, study, staff\n",
      "\n",
      "Topic  2\n",
      "warehouse, database, data, data technologies, azure, pipelines data, databricks, technologies, pipelines, platform, manufacturing, solutions, solutions fortune, process warehouse, excell com, azure visas, relationships consultants, team database, database background, data azure\n",
      "\n",
      "Topic  3\n",
      "cell, patients, medicine, therapy, cell therapy, study, system, cancer, trial, receptors, research, patient, patient experience, therapy patient, information, ability, partners, trials, studies, problem\n",
      "\n",
      "Topic  4\n",
      "cell, patients, client, medicine, cell therapy, therapy, cancer, manufacturing, receptors, customer, program, clients, hands, patient, data science, warehouse, patient experience, therapy patient, partners, teams\n",
      "\n",
      "Topic  5\n",
      "client, program, service, changes, clients, search, manager, members, query, market, quality, team members, projects clients, client service, partnerships, business, data, time, entities, jobs\n",
      "\n",
      "Topic  6\n",
      "query, entities, study, changes, search, research, user, guidelines, ambiguities changes, web information, guideline ambiguities, research web, ambiguity errors, problem concise, information entities, errors problems, concise way, time tasks, guideline, ambiguities\n",
      "\n",
      "Topic  7\n",
      "study, execution, trial, vendors, documents, studies, sites, study documents, client, opportunities, communications, files, duties, trial assistant, initiation, ability information, project teams, trials, assistant, management\n",
      "\n",
      "Topic  8\n",
      "fish, player, creativity, collaboration, time, business stakeholders, partner, sql, visualizations, type time, warehouse, benefits, query, study, analysts, life, stakeholders, entities, data science, opportunities\n",
      "\n",
      "Topic  9\n",
      "program, client, transportation, changes, clients, manager, relationship, customer, success, account, trends, notes, manage, strategies, customer service, recommendations, information, team, programs, methods\n",
      "\n",
      "Topic  10\n",
      "search, analytics, team, data scientist, data sets, study, sets, fitness, scientist, award, employee, ownership, status, benefits compensation, downtown, color origin, status disability, benefits, age status, assets\n",
      "\n",
      "Topic  11\n",
      "customers, connections, network, communities, applications, companies, world, deployment, fish, services, queries, solutions, player, industry, provider, providers, communications, energy, environments, range\n",
      "\n",
      "Topic  12\n",
      "models, status, model, expert, machine, energy, performance, communications, business, strategy, origin, system, applicants, color, improvement, ownership, data business, initiative, preparation, building\n",
      "\n",
      "Topic  13\n",
      "drawings, specifications, contract, codes, status, deliverables, law, safety, employment, business software, management, requirements, review, construction, communications, files, applicants, network, deployment, communities\n",
      "\n",
      "Topic  14\n",
      "traffic, energy, staff, employees, contract, area, agency, transit, safety, sound, services, principles, managers, state, consultants, analytics, pay, sound transit, responsibility, candidates\n",
      "\n",
      "Topic  15\n",
      "energy, drawings, specifications, employment, accommodation, forecasting, status, optimization, codes, contract, position, skills position, deliverables, customers, schedule, construction, product, services, office, products\n",
      "\n",
      "Topic  16\n",
      "energy, files, communications, location, preparation, employer, employees, site, system, training materials, sex age, information, trial assistant, participates, benefits, bachelors experience, team coordination, coordination, assistant, departments\n",
      "\n",
      "Topic  17\n",
      "opportunities, requests, correspondence, contact, companies, employment, trials, operations, documents, world, familiarity, career, meetings meeting, administration, operations team, veterans, today, word, industries, support\n",
      "\n",
      "Topic  18\n",
      "documentation, user, training, members, principles, end, detail, applications, customer service, description, standards, organizations, health, requests, organization, security, project team, customer, stakeholders, project manager\n",
      "\n",
      "Topic  19\n",
      "site, certification, challenges, engineering, cost, career, consulting, engineers, project, use, elements, calculations, eit, project manager, sewer, performance, experience engineer, people, commitment, program\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, vectorizer.get_feature_names(), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:34:51.959644Z",
     "start_time": "2019-11-09T06:34:50.904474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "datum, data, data scientist, business, analytic\n",
      "\n",
      "Topic  1\n",
      "engineering, design, water, drawing, construction\n",
      "\n",
      "Topic  2\n",
      "datum, warehouse, process warehouse, database, datum technology\n",
      "\n",
      "Topic  3\n",
      "patient, cell, therapy, medicine, cancer\n",
      "\n",
      "Topic  4\n",
      "datum, datum science, hand, datum scientist, team\n",
      "\n",
      "Topic  5\n",
      "client, project, candidate, sector, position\n",
      "\n",
      "Topic  6\n",
      "query, ambiguity, guideline, search, change\n",
      "\n",
      "Topic  7\n",
      "study, trial, document, vendor, execution\n",
      "\n",
      "Topic  8\n",
      "fish, player, visualization, datum, business\n",
      "\n",
      "Topic  9\n",
      "client, program, program manager, manager, change\n"
     ]
    }
   ],
   "source": [
    "# NMF\n",
    "\n",
    "nmf_model = NMF(10)\n",
    "doc_topic = nmf_model.fit_transform(noun_vc)\n",
    "\n",
    "display_topics(nmf_model, vectorizer.get_feature_names(), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:40:23.313338Z",
     "start_time": "2019-11-09T06:40:20.067710Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1142"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NMF: noun_lemma\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                            max_df = 0.8,\n",
    "                            min_df = 0.05,\n",
    "                            stop_words = ['yearexperience', 'year', 'years', 'porch', 'work', 'home', \n",
    "                                          'term', 'datum', 'hand', 'science'])\n",
    "\n",
    "noun_vc = vectorizer.fit_transform(jobs.noun_lemma)\n",
    "\n",
    "len(vectorizer.vocabulary_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:40:25.807881Z",
     "start_time": "2019-11-09T06:40:24.927928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "engineering, water, design, utility, firm, construction, site, drawing, grading, survey\n",
      "\n",
      "Topic  1\n",
      "scientist, customer, learning, model, business opportunity, revenue, statistic, machine learning, product, homeowner\n",
      "\n",
      "Topic  2\n",
      "warehouse, process warehouse, database, databrick, azure, pipeline, solution, technology, process, format auditing\n",
      "\n",
      "Topic  3\n",
      "fish, player, visualization, analyst, business, creativity, stakeholder, talent, business stakeholder, type time\n",
      "\n",
      "Topic  4\n",
      "patient, cell, therapy, medicine, cancer, cell therapy, receptor, manufacturing, patient experience, therapy patient\n",
      "\n",
      "Topic  5\n",
      "client, program, program manager, manager, change, transportation, customer, success, service, client relationship\n",
      "\n",
      "Topic  6\n",
      "query, ambiguity, guideline, change, search, user, entity, research, research web, guideline ambiguity\n",
      "\n",
      "Topic  7\n",
      "study, trial, vendor, execution, study trial, protocol, study site, management, travel, scope\n",
      "\n",
      "Topic  8\n",
      "search, analytic, data scientist, data, technology, scientist, set, asset, production, platform\n",
      "\n",
      "Topic  9\n",
      "client, project, candidate, sector, position, firm, autocad, quality, job, team member\n",
      "\n",
      "Topic  10\n",
      "provider, network, connection, community, customer, world, service, deployment, service provider, payment\n",
      "\n",
      "Topic  11\n",
      "model, business, visualization, machine, expert, process, tool, performance, azure, role\n",
      "\n",
      "Topic  12\n",
      "award, design, transportation, water, status, ownership, project, water resource, region, resource\n",
      "\n",
      "Topic  13\n",
      "drawing, specification, contract, employment, review, status, safety, construction, deliverable, requirement\n",
      "\n",
      "Topic  14\n",
      "energy, customer, solution, forecasting, optimization, accommodation, application, service, analytic, product\n",
      "\n",
      "Topic  15\n",
      "traffic, engineering, staff, design, agency, contractor, duty, employee, state, matter\n",
      "\n",
      "Topic  16\n",
      "study, document, information, file, department, preparation, status, employer, location, benefit\n",
      "\n",
      "Topic  17\n",
      "trial, study, opportunity, today, world, document, correspondence, contact, request, employment opportunity\n",
      "\n",
      "Topic  18\n",
      "user, training, application, documentation, organization, customer, project, business, member, end\n",
      "\n",
      "Topic  19\n",
      "type time, algorithm, data scientist, recommendation, pipeline, delivery, scientist, data, solution, type\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(20)\n",
    "doc_topic = nmf_model.fit_transform(noun_vc)\n",
    "\n",
    "display_topics(nmf_model, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:40:33.295743Z",
     "start_time": "2019-11-09T06:40:33.286876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['company_name', 'description', 'job_title', 'link', 'location',\n",
       "       'salary', 'type', 'clean_text', 'noun', 'noun_lemma', 'lemma', 'word'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:51:41.963853Z",
     "start_time": "2019-11-09T06:51:39.304884Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1124"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2),\n",
    "                            max_df = 0.6,\n",
    "                            min_df = 0.05,\n",
    "                            stop_words = ['yearexperience', 'year', 'years', 'porch', 'work', 'home', \n",
    "                                          'term', 'datum', 'hand', 'science', 'other', 're', 'sex'])\n",
    "\n",
    "noun_lemma_vc = vectorizer.fit_transform(jobs.noun_lemma)\n",
    "\n",
    "len(vectorizer.vocabulary_) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:51:43.491499Z",
     "start_time": "2019-11-09T06:51:43.103795Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00869464, 0.08902728, 0.071433  , 0.06354637, 0.05619893,\n",
       "       0.05441557, 0.05237732, 0.04665867, 0.04360922, 0.03971836,\n",
       "       0.03741997, 0.03478475, 0.03103652, 0.0270211 , 0.02460624,\n",
       "       0.02390971, 0.02232453, 0.01892156, 0.01646111, 0.01380205])"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = 20\n",
    "lsa_n_l = TruncatedSVD(components)\n",
    "\n",
    "\n",
    "noun_topics = lsa_n_l.fit_transform(noun_vc.toarray())  # reduce dimensionality\n",
    "\n",
    "lsa_n_l.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-09T06:51:44.410740Z",
     "start_time": "2019-11-09T06:51:44.363818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "provider center, business analyst, client job, engineering design, design site, customer problem, simulation, modeling machine, optimization, engineer passion\n",
      "\n",
      "Topic  1\n",
      "sector building, modeling machine, story, leukemia, customer problem, business owner, salary, management software, management system, business analyst\n",
      "\n",
      "Topic  2\n",
      "writing, database scientist, product treatment, azure, day, thing industry, plan drawing, product owner, specialist associate, database\n",
      "\n",
      "Topic  3\n",
      "patient vision, cell manufacturing, support principle, flexibility business, user acquisition, timeline, world medicine, cancer accomplishment, medicine system, position communication\n",
      "\n",
      "Topic  4\n",
      "patient vision, cell manufacturing, timeline, medicine system, cancer accomplishment, center, user acquisition, regression, client job, project management\n",
      "\n",
      "Topic  5\n",
      "client job, project management, change assignment, service solution, receptor, ambiguity, environment service, project team, client service, visit excell\n",
      "\n",
      "Topic  6\n",
      "service solution, receptor, ambiguity, health, environment service, visit excell, retirement, support principle, check, ambiguity error\n",
      "\n",
      "Topic  7\n",
      "support principle, user acquisition, water resource, experience analytic, drainage, system cancer, design, survey, communicator, mentorship\n",
      "\n",
      "Topic  8\n",
      "analytic, database development, database, engineer data, asset, policy system, service solution, thing industry, program information, simulation\n",
      "\n",
      "Topic  9\n",
      "project management, client job, project team, change assignment, customer problem, type company, map, user, support, data\n",
      "\n",
      "Topic  10\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-456-7050b3995534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsa_n_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-412-9bbb74a704ef>\u001b[0m in \u001b[0;36mdisplay_topics\u001b[0;34m(model, feature_names, no_top_words, topic_names)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTopic: '\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         print(\", \".join([feature_names[i]\n\u001b[0;32m----> 8\u001b[0;31m                         for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-412-9bbb74a704ef>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTopic: '\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopic_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         print(\", \".join([feature_names[i]\n\u001b[0;32m----> 8\u001b[0;31m                         for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "display_topics(lsa_n_l, vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T21:14:36.615557Z",
     "start_time": "2019-11-08T21:14:36.599162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Resume1</th>\n",
       "      <th>Resume2</th>\n",
       "      <th>Resume3</th>\n",
       "      <th>Resume4</th>\n",
       "      <th>Resume5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Job1</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Job2</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Job3</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Job4</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Job5</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Resume1  Resume2  Resume3  Resume4  Resume5\n",
       "Job1     0.54     0.61     0.47     0.48     0.52\n",
       "Job2     0.99     1.00     0.98     0.95     0.99\n",
       "Job3     0.81     0.79     0.89     0.68     0.83\n",
       "Job4     0.54     0.61     0.47     0.48     0.52\n",
       "Job5     1.00     0.99     0.99     0.97     1.00"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic = lsa.fit_transform(doc_word)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project4e]",
   "language": "python",
   "name": "conda-env-project4e-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
